{
  "hash": "4bebae2993d16cb80ba746283c603fb2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analysis of Frequencies\"\nsubtitle: Lecture 10\nformat:\n  revealjs:\n    theme: clean.scss\n    scrollable: true\n    footer: \"Lecture 10 - [Back to home](https://madfudolig.quarto.pub/introtobiostats/)\"\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    slide-number: true\n    menu: true\n    code-annotations: hover\n    chalkboard: true\n    engine: knitr\n    echo: true\n    code-fold: false\n    \n  pdf: \n    number-sections: true\n    geometry:\n      - landscape\n      - margin=1in\n    include-in-header:\n      text: |\n        \\usepackage{titlesec}\n        \\titleformat{\\section}{\\normalfont\\large\\bfseries}{\\thesection}{1em}{}\n        \\titleformat{\\subsection}{\\normalfont\\large\\bfseries}{\\thesubsection}{1em}{}\n        \\titleformat{\\subsubsection}{\\normalfont\\normalsize\\bfseries}{\\thesubsubsection}{1em}{}\n        \n        % Syntax: \\titlespacing{command}{left-sep}{before-sep}{after-sep}\n        \\titlespacing{\\section}{0pt}{12pt}{12pt}\n        % \\titlespacing{\\subsection}{0pt}{3in}{12pt}\n        \\titlespacing{\\subsubsection}{0pt}{3in}{10pt}\n        \n        % This forces a clearpage before every subsection\n        \\let\\oldsubsection\\subsection\n        \\renewcommand{\\subsection}{\\clearpage\\oldsubsection}\n        \nbibliography: references.bib\n---\n\n# Outline\n\n-   Introduction to Categorical Data\n-   Goodness-of-Fit Tests\n-   Test of Homogeneity\n-   Test of Independence\n-   Fisher's Exact Test\n-   Symmetry Tests\n-   Measures of Association\n\n# Categorical Data\n\n## Associations between Categorical Data\n\nOften, we are interested in testing for association between categorical data. These variables can be nominal or ordinal.\n\n::: callout-note\nSuppose we want to test whether there is an association between the type of hospital a patient is admitted to and their diagnosed conditions.\n:::\n\n## Contingency Tables\n\nA contingency table is a table of frequencies or counts for each possible combination of the variables.\n\n::: callout-note\n## Example\n\nThe hair and eye color of male statistics students were measured. The findings were summarized in the following contingency table.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n```\n\n\n:::\n:::\n\n:::\n\n## Contingency tables in `R`\n\nYou can use `xtabs()` to produce contingency tables in `R`. The data should be formatted such that the two variables are defined by two columns. If these columns are `x` and `y` from a data frame `df`, the sample code would look like:\n\n``` r\nxtabs(~x+y,data=df)\n```\n\n`x` will be assigned as the row variable, `y` will be assigned as the column variable.\n\n## Example\n\nThe data set `penguins` preloaded in `R` includes data from penguins at the Palmer Archipelago in Antarctica.\n\n::: panel-tabset\n### Question\n\nCreate a contingency table for this data that summarizes the number of penguins belonging to each species and sex.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(~species+sex,data=penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           sex\nspecies     female male\n  Adelie        73   73\n  Chinstrap     34   34\n  Gentoo        58   61\n```\n\n\n:::\n:::\n\n:::\n\n## Chi-Squared Distribution\n\nTests involving categorical data use the chi-squared distribution to approximate the distribution of the test statistics.\n\n::: panel-tabset\n### Support\n\nThe chi-squared distribution is defined for non-negative values $(0,\\infty)$.\n\n### Parameter\n\nThe chi-squared distribution can be defined by the degrees of freedom $\\nu$ or $df$.\n\nLike the t-distribution, we only need one value for the degree of freedom.\n\n### Relation to Gaussian Distribution\n\nSuppose $Z$ follows the standard Gaussian distribution N(0,1). Then $Z^2$ follows a chi-square distribution with 1 degree of freedom.\n\n### Mean and Variance\n\nThe mean of the chi-squared distribution is $k$, and the variance of the chi-squared distribution is $2k$.\n:::\n\n# Goodness-of-Fit Tests\n\n## Goodness of Fit\n\nSuppose we want to test if the data follows a specified distribution.\n\n::: callout-note\n## Example\n\nSuppose we want to know if a six-sided die is fair. We would expect to roll the numbers uniformly after a large number of throws. However, there will be variability due to randomness. The goodness-of-fit test will provide information if we have evidence of deviating from the pre-specified uniform distribution.\n:::\n\n## Hypothesis Test\n\nThe null hypothesis of the goodness-of-fit test is that **the observed data follows the specified distribution**, while the alternative is that it does not follow the specified distribution.\n\n## Test Statistic\n\nSuppose there are $k$ bins separating the data and that the distribution provides an expected number/counts of events $E_i$ for $i=1,2,...k$. If the observed number/counts in the data is $O_i$, we define the test statistics $Q$ such that\n\n$$\nQ = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n$$\n\n::: callout-important\nQ approximately follows the chi-squared distribution with $k-1$ degrees of freedom, denoted by $\\chi^2_{k-1}$.\n\nFor the chi-squared distribution assumption to be valid, the expected value of each bin should be greater than 5.\n:::\n\n## p-value calculation\n\nThe p-values can be calculated using the chi-squared distribution such that:\n\n$$\np-value = P(\\chi^2_{k-1} \\geq Q)\n$$\n\n## `R` implementation\n\nThere are two ways to check for goodness of fit: formal statistical analysis and exploratory data visualization.\n\n:::: panel-tabset\n### Expected Values\n\nFor discrete distributions, the expected values can be calculated using the PDF functions ( `dpois`, `dbinom`) for discrete distributions, and CDF functions (`pnorm`,`punif`, `pt`).\n\n### Formal Statistical Analysis\n\nThe `chisq.test` function performs the chi-squared test of goodness of fit. The function needs a vector of the observed variables, `x_observed` and a vector of probabilities `p`.\n\n``` r\nchisq.test(x=x_observed, p=p)\n```\n\n::: callout-important\nThe `chisq.test()` function includes a continuity correction in calculating the test statistic and the corresponding p-value. If we want the uncorrected statistic and p-value, we need to specify `correct=FALSE`.\n:::\n\n### Visualization\n\nOnce the expected values are calculated, we can plot these values using `ggplot()`. \n::::\n\n## Example\n\nElectronic integrated circuits are produced from thin wafers that are cut from some material. The wafers produced sometimes have tiny flaws on them that make part of the wafer unusable. Suppose we produce 1000 wafers and for each we determine the number of flaws.\n\n::: panel-tabset\n### Data\n\n| Number of Flaws | Observed Frequency |\n|-----------------|--------------------|\n| 0               | 10                 |\n| 1               | 220                |\n| 2               | 130                |\n| 3               | 80                 |\n| 4               | 60                 |\n| 5 or more       | 90                 |\n\n### Question\n\nTest whether these data follow a Poisson distribution with $\\lambda=1.44$. Visualize the observed and expected counts to support the results of the test.\n\n### Answer\n\nWe specify the observed variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved <- c(10,220,130,80,60,90)\nobserved\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  10 220 130  80  60  90\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal <- sum(observed)\ntotal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 590\n```\n\n\n:::\n:::\n\n\nThe specified distribution is the Poisson distribution. We can then calculate the expected probabilities using `dpois` and `ppois`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpected <- c(\ndpois(0,lambda=1.44),\ndpois(1,lambda=1.44),\ndpois(2,lambda=1.44),\ndpois(3,lambda=1.44),\ndpois(4,lambda=1.44),\n1-ppois(4,lambda=1.44)\n)\nexpected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.23692776 0.34117597 0.24564670 0.11791042 0.04244775 0.01589140\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(expected) # must be 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal*expected # there should not be more than 1.5 bins that have less than 5 expected counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 139.787378 201.293824 144.931553  69.567145  25.044172   9.375928\n```\n\n\n:::\n:::\n\n\nAll expected counts are above 5. We can now use the chi-square approximation for the p-value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest <- chisq.test(x=observed,p=expected)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed\nX-squared = 867.42, df = 5, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe test statistic is 867.4246622 with a p-value \\< 2.2e-16. We reject the null hypothesis. We have sufficient evidence to conclude that the observed data does not follow the Poisson distribution.\n\n### Plot\n\nThe function `bind_rows` appends the expected data frame to the observed data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\ndf_observed <- data.frame(bin = c(0,1,2,3,4,\"5 or more\"),y=observed,type=\"Observed\")\ndf_expected <- data.frame(bin = c(0,1,2,3,4,\"5 or more\"),y=total*expected,type=\"Expected\")\ndf <- bind_rows(df_observed,df_expected)\n\nggplot(df,aes(x=bin,y=y,group=type,fill=type)) +\n  geom_bar(position=\"dodge\",stat=\"identity\") + \n  theme_bw() + \n  labs(x=\"Number of failures\", y=\"Count\",fill=\"Observed vs. Expected\")\n```\n\n::: {.cell-output-display}\n![](lecture10_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n:::\n\n## Exercise\n\nIn the “nighttime” lottery run by the state of Texas, three numbers are selected from the digits 0 through 9. The frequencies of the first digit selected over a period of almost 30 years (from 1993 to 2023) are shown below for each of the 9,215 days.\n\n::: panel-tabset\n### Data Set\n\n| Digit | Frequency |\n|-------|-----------|\n| 0     | 918       |\n| 1     | 905       |\n| 2     | 908       |\n| 3     | 916       |\n| 4     | 900       |\n| 5     | 911       |\n| 6     | 963       |\n| 7     | 948       |\n| 8     | 937       |\n| 9     | 909       |\n\n### Question\n\nTest whether each digit is equally likely to have been selected in the Texas \"nighttime\" lottery.\n\n### Answer\n\n\nWe specify the observed variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved <- c(918,905,908,916,900,911,963,948,937,909)\nobserved\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 918 905 908 916 900 911 963 948 937 909\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal <- sum(observed)\ntotal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9215\n```\n\n\n:::\n:::\n\n\nThe specified distribution is the discrete uniform distribution. We can then calculate the uniform probabilities as 1/10 (10 bins).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpected <- c(1/10,1/10,1/10,1/10,1/10,1/10,1/10,1/10,1/10,1/10\n)\nexpected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(expected) # must be 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal*expected # there should not be more than 1.5 bins that have less than 5 expected counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 921.5 921.5 921.5 921.5 921.5 921.5 921.5 921.5 921.5 921.5\n```\n\n\n:::\n:::\n\n\nAll expected counts are above 5. We can now use the chi-square approximation for the p-value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest <- chisq.test(x=observed,p=expected)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed\nX-squared = 4.2219, df = 9, p-value = 0.8962\n```\n\n\n:::\n:::\n\n\nThe test statistic is 4.2219208 with a p-value 0.8962084. We fail to reject the null hypothesis. We have insufficient evidence to conclude that the digits are not equally likely to be chosen for the \"nighttime\" lottery.\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf_observed <- data.frame(bin = 0:9,y=observed,type=\"Observed\")\ndf_expected <- data.frame(bin = 0:9,y=total*expected,type=\"Expected\")\ndf <- bind_rows(df_observed,df_expected)\n\nggplot(df,aes(x=as.factor(bin),y=y,group=type,fill=type)) +\n  geom_bar(position=\"dodge\",stat=\"identity\") + \n  theme_bw() + \n  labs(x=\"Number of failures\", y=\"Count\",fill=\"Observed vs. Expected\")\n```\n\n::: {.cell-output-display}\n![](lecture10_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n\n# Test of Independence\n\n## Test of Independence\n\nSuppose we are interested in a contingency table made from two distinct categorical variables. \n\n::: callout-note\n## Assumptions\n\nSuppose the row variables has $r$ levels and the column variables has $c$ levels. This implies that any unit/subject can fall in any of the $rc$ categories.\n:::\n\n## Hypothesis Test\n\nThe null hypothesis for the test of independence is that **the row and column variables are independent of each other**, while the alternative hypothesis is that the row and column variables are not independent.\n\n## Null Hypothesis Assumption\n\nSuppose we define $p_{ij}$ as the probability of a unit to be found at row $i$ and column $j$, denoted by $(i,j)$. We also define $p_{i\\cdot}$ and $p_{\\cdot j}$ as the respective probabilities of finding the unit at row $i$ and finding the unit at row $j$. The assumption of independence implies that for all $i$ and $j$,\n\n$$\np_{ij} = p_{i\\cdot} p_{\\cdot j}\n$$\n\n::: callout-note\nUnder the assumption that the null hypothesis is true, the expected counts for each cell can be calculated as:\n\n$$\nE_{ij} = r_ic_j/n\n$$\n\nwhere $r_i$ is the row total for row $i$, $c_j$ is the column total for column $j$, and $n$ is the total sample size.\n:::\n\n## Test Statistic\n\nThe test statistic compares the deviation of the observed data to the expected data similar to the goodness-of-fit.\n\n$$\nQ = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n$$\n\nThe test statistic approximately follows a chi-squared distribution with degrees of freedom $(r-1)(c-1)$, $\\chi^2_{(r-1)(c-1)}$ where $r$ and $c$ are the total numbers of rows and columns, respectively.\n\n::: callout-important\nIt is important to check the expected number of events for each cell. If the expected number of events is less than 5 for 25% of the cells, the chi-square distribution does not hold.\n:::\n\n\n\n## p-value\n\nThe p-value can be calculated using the probability $P(\\chi^2_{(r-1)(c-1)} \\geq Q)$.\n\n\n## `R` implementation\n\nThe `chisq.test()` can also be used to perform tests of independence.\n\n::: callout-note\nWhen dealing with data sets, it would be beneficial to create a contingency table with `xtabs()` first to check for data sparsity (low cell counts) before using it as an input in `chisq.test()`.\n:::\n\n```{.r}\nxtabs(~x+y, data=df)\nchisq <-chisq.test(df$x,df$y)\nchisq\n```\n\n\n::: callout-tip\n`chisq.test()` calculates the expected values using the `chisq$expected` option.\n:::\n\n## Example\n\nConsider the sleep health data, `SleepHealthData.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep <- read.csv(\"SleepHealthData.csv\")\n```\n:::\n\n\n::: panel-tabset\n### Question\n\nTest whether gender (variable `gender`) is associated with reported sleep disorders (variable `sleep_disorder`. Use a significance level of 0.01.\n\n### Answer\n\nCheck for sparsity of data first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(~gender+sleep_disorder,data=sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        sleep_disorder\ngender   Insomnia None Sleep Apnea\n  Female       36   82          67\n  Male         41  137          11\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest <- chisq.test(sleep$gender,sleep$sleep_disorder)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  sleep$gender and sleep$sleep_disorder\nX-squared = 54.306, df = 2, p-value = 1.613e-12\n```\n\n\n:::\n\n```{.r .cell-code}\nchisqtest$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            sleep$sleep_disorder\nsleep$gender Insomnia     None Sleep Apnea\n      Female 38.08824 108.3289    38.58289\n      Male   38.91176 110.6711    39.41711\n```\n\n\n:::\n:::\n\n\nAll expected values are above 5, which means the chi-squared assumption is valid. The p-value is \\ensuremath{1.6128634\\times 10^{-12}}, the test statistic is 54.3060201 with 2 degrees of freedom. At a significance level of 0.01, we reject the null hypothesis. We have sufficient evidence to conclude that the gender and occurrence of sleep disorder are not independent.\n\n\n:::\n\n\n## Exercise\n\n\nConsider the sleep health data, `SleepHealthData.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep <- read.csv(\"SleepHealthData.csv\")\n```\n:::\n\n\n::: panel-tabset\n### Question\n\nTest whether stress level (variable `stress_level`) is associated with reported sleep disorders (variable `sleep_disorder`). Treat stress level as an ordinal categorical variable. Use a significance level of 0.01.\n\n### Answer\n\n\nCheck for sparsity of data first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(~stress_level+sleep_disorder,data=sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            sleep_disorder\nstress_level Insomnia None Sleep Apnea\n           3        1   40          30\n           4       24   43           3\n           5        6   57           4\n           6        2   43           1\n           7       41    3           6\n           8        3   33          34\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest <- chisq.test(sleep$stress_level,sleep$sleep_disorder)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  sleep$stress_level and sleep$sleep_disorder\nX-squared = 240.2, df = 10, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nchisqtest$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  sleep$sleep_disorder\nsleep$stress_level  Insomnia     None Sleep Apnea\n                 3 14.617647 41.57487   14.807487\n                 4 14.411765 40.98930   14.598930\n                 5 13.794118 39.23262   13.973262\n                 6  9.470588 26.93583    9.593583\n                 7 10.294118 29.27807   10.427807\n                 8 14.411765 40.98930   14.598930\n```\n\n\n:::\n:::\n\n\nAll expected values are above 5, which means the chi-squared assumption is valid. The p-value is \\ensuremath{6.2217174\\times 10^{-46}}, the test statistic is 240.1993685 with 10 degrees of freedom. At a significance level of 0.01, we reject the null hypothesis. We have sufficient evidence to conclude that the stress level and occurrence of sleep disorder are not independent.\n\n:::\n\n# Test of Homogeneity\n\n## Test of Homogeneity\n\nSuppose the sampling strategy was to select a fixed sample size per group, and measure the response for each unit/individual.\n\n::: callout-note\nIf the samples are stratified per group, the groups are not random. The response remains random. The margins for the grouping variable, typically assigned in the rows, are fixed in advance.\n:::\n\n## Homogeneity vs. Independence\n\n::: callout-important\nBecause people assigned to/sampled from groups are no longer random, there is no sense to test for independence. Instead, we test whether the probability of being in each of the outcome groups is the same across all treatments.\n\nHence, the test is referred to as the **test of homogeneity**.\n:::\n\n## Hypothesis Test\n\nThe null hypothesis is that the marginal probability of the outcome variable is the same across all the groups/populations/treatments considered in the study.\n\nThe alternative hypothesis is that at least one marginal probability is not equal to the others.\n\n::: callout-note\nThis test is similar to comparing the difference of two proportions for a 2x2 contingency table.\n:::\n\n## Null Hypothesis Assumption\n\nSuppose we define $p_{ij}$ as the probability of a unit to be found at row $i$ and column $j$, denoted by $(i,j)$. The assumption of homogeneity assumes that the marginal probability across the columns are the same. Hence,\n\n$$\np_{ij} = c_j/n\n$$\n\n::: callout-note\nUnder the assumption that the null hypothesis is true, the expected counts for each cell can be calculated as:\n\n$$\nE_{ij} = r_ip_{ij} = r_i(c_j/n)\n$$\n\nwhere $r_i$ is the row total for row $i$, $c_j$ is the column total for column $j$, and $n$ is the total sample size.\n:::\n\n## Test Statistic\n\nThe test statistic compares the deviation of the observed data to the expected data similar to the goodness-of-fit.\n\n$$\nQ = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n$$\n\nThe test statistic approximately follows a chi-squared distribution with degrees of freedom $(r-1)(c-1)$, $\\chi^2_{(r-1)(c-1)}$ where $r$ and $c$ are the total numbers of rows and columns, respectively.\n\n::: callout-important\nNote that the test statistic is the same for independence and homogeneity tests, but the assumptions and hypotheses are different.\n\nIt is important to check the expected number of events for each cell. If the expected number of events is less than 5 for 25% of the cells, the chi-square distribution does not hold.\n:::\n\n## p-value Calculation\n\nThe p-value can be calculated using the probability $P(\\chi^2_{(r-1)(c-1)} \\geq Q)$.\n\n## `R` Implementation\n\nThe `R` implementation of the homogeneity test is similar to that of the independence test.\n\n## Example\n\nThe data set `covid.csv` includes data from a clinical trial for treatments of COVID-19. Suppose that subjects are randomly assigned to one of two treatments: an experimental drug and a placebo (best available treatment).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ncovid <- read.csv(\"datasets/covid.csv\")\nglimpse(covid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,297\nColumns: 3\n$ ID        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~\n$ Treatment <chr> \"Experimental\", \"Experimental\", \"Experimental\", \"Experimenta~\n$ Outcome   <chr> \"Not Hospitalized\", \"Not Hospitalized\", \"Not Hospitalized\", ~\n```\n\n\n:::\n:::\n\n\n\n::: panel-tabset\n### Question\n\nWe want to test whether the marginal probabilities of each outcome (not hospitalized, hospitalized, and died) are the same across each treatment. Use a significance level of 0.05.\n\n### Answer\n\nCreate a contingency table first to test whether there are sparse counts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontingency <- xtabs(~Treatment+Outcome,data=covid)\ncontingency\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Outcome\nTreatment      Died Hospitalized Not Hospitalized\n  Experimental   11           67              572\n  Placebo        23           81              543\n```\n\n\n:::\n:::\n\n\nWe implement the chi-squared test using `chisq.test()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest <- chisq.test(covid$Treatment,covid$Outcome)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  covid$Treatment and covid$Outcome\nX-squared = 6.307, df = 2, p-value = 0.0427\n```\n\n\n:::\n\n```{.r .cell-code}\nchisqtest$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               covid$Outcome\ncovid$Treatment     Died Hospitalized Not Hospitalized\n   Experimental 17.03932     74.17116         558.7895\n   Placebo      16.96068     73.82884         556.2105\n```\n\n\n:::\n\n```{.r .cell-code}\n# OR\n\nchisqtest <- chisq.test(contingency)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  contingency\nX-squared = 6.307, df = 2, p-value = 0.0427\n```\n\n\n:::\n:::\n\n\nAll expected values are above 5, which means the chi-squared assumption is valid. The p-value is 0.042703, the test statistic is 6.3069732 with 2 degrees of freedom. At a significance level of 0.05, we reject the null hypothesis. We have sufficient evidence to conclude that the marginal probabilities of the outcome variable differ across treatments.\n\n:::\n\n## Exercise\n\nThe following data is from a study investigating the sources of health information from 200 urban young adults and 150 rural young adults.\n\n::: panel-tabset\n\n### Data\n\nThis is how to input aggregated data in `R`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource_data <- data.frame(\n  Location = c(\"Urban\", \"Urban\",\"Urban\", \"Rural\", \"Rural\",\"Rural\"),\n  Source = c(\"Social Media\", \"Medical Professionals\", \"Others\", \"Social Media\", \"Medical Professionals\", \"Others\"),\n  Count = c(84,96,20,17,31,53)\n)\n\n# Create a cross-table using xtabs\ncross_table_xtabs <- xtabs(Count ~ Location + Source, data = source_data)\ncross_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Source\nLocation Medical Professionals Others Social Media\n   Rural                    31     53           17\n   Urban                    96     20           84\n```\n\n\n:::\n:::\n\n\n### Question\n\nTest whether the marginal probabilities are homogeneous across the urban and rural participants.\n\n\n### Answer\n\nWe can use the contingency table `cross_table_xtabs` in the function `chisq.test()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisqtest<- chisq.test(cross_table_xtabs)\nchisqtest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  cross_table_xtabs\nX-squared = 67.356, df = 2, p-value = 2.365e-15\n```\n\n\n:::\n\n```{.r .cell-code}\nchisqtest$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Source\nLocation Medical Professionals   Others Social Media\n   Rural              42.61462 24.49502     33.89037\n   Urban              84.38538 48.50498     67.10963\n```\n\n\n:::\n:::\n\n\nAll expected counts are above 5, hence we can use the chi-squared assumption. All expected values are above 5, which means the chi-squared assumption is valid. The p-value is \\ensuremath{2.3649711\\times 10^{-15}}, the test statistic is 67.3560212 with 2 degrees of freedom. At a significance level of 0.05, we reject the null hypothesis. We have sufficient evidence to conclude that the marginal probabilities are not homogeneous across the rurality of residence of the participants.\n:::\n\n# Fisher Exact Test\n\n## Chi-Squared Tests: Small Sample Size\n\nFor small sample sizes, the expected values for the cells might be less than 5 for most cells. The chi-squared assumption might not be valid for these cases.\n\n## Fisher Exact Test\n\nThe Fisher Exact Test calculates an *exact* p-value using the hypergeometric distribution.\n\n::: callout-important\nBecause the Fisher Exact Test does not rely on any asymptotic (long-run) behavior of the random variables, there is no requirement for a minimum sample size.\n\nThe Fisher Exact Test is used primarily for 2x2 tables with small cell counts.\n:::\n\n## Hypothesis Test \n\nThe hypothesis tests for the test of independence and homogeneity still hold.\n\n::: callout-note\nUnlike the chi-squared tests, the alternative hypothesis can be one-sided.\n\n-   One sided $H_a$: There is a positive/negative association between the row and column variables. OR One combination of the row and column variable levels is more favored compared to the others.\n-   Two-sided $H_a$: there is an association between the row and column variables.\n:::\n\n## Test statistic and p-value\n\nThe p-value is calculated based on the probability of the configuration of the contingency table. The test assumes that the row and column totals are fixed.\n\n::: callout-note\nThe test statistic used is the upper left cell of the contingency table, $X$. \n\nThe hypergeometric distribution is used to determine which numbers are more extreme than what was observed.\n\nOnce the values are established, we sum the probabilities of all the values that are as extreme or more extreme than what was observed.\n:::\n\n## `R` implementation\n\nThe function `fisher.test()` can be used to perform the Fisher Exact Test. It can use the data frame or a cross table to calculate the exact p-values.\n\n### Sample Code\n\n```{.r}\nfisher.test(df,alternative=\"greater\")\n\n# OR\n\nfisher.test(contingency,alternative=\"two.sided\")\n```\n\n## Example\n\nThe table shown shows data from a study on treatments for healing severe infections.\n\n::: panel-tabset\n### Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregated_data <- data.frame(\n  Treatment = c(\"A_Test\", \"A_Test\", \"B_Control\", \"B_Control\"),\n  Outcome = c(\"Favorable\", \"Unfavorable\", \"Favorable\", \"Unfavorable\"),\n  Count = c(10,2,2, 4)\n)\n\n# Create a cross-table using xtabs\ncross_table_xtabs <- xtabs(Count ~ Treatment + Outcome, data = aggregated_data)\ncross_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Outcome\nTreatment   Favorable Unfavorable\n  A_Test           10           2\n  B_Control         2           4\n```\n\n\n:::\n:::\n\n\n### Question\n\nUse the Fisher's Exact Test to see if there is a positive association between the presence of the test treatment and the outcome. Use a significance level of 0.10.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher<- fisher.test(cross_table_xtabs,alternative=\"greater\")\nfisher\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  cross_table_xtabs\np-value = 0.05726\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.9374086       Inf\nsample estimates:\nodds ratio \n  8.457238 \n```\n\n\n:::\n:::\n\n\nCompare the result to asymptotic chi-squared tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(cross_table_xtabs)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(cross_table_xtabs): Chi-squared approximation may be\nincorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  cross_table_xtabs\nX-squared = 2.5312, df = 1, p-value = 0.1116\n```\n\n\n:::\n:::\n\n\n\n\nThe resulting p-value from the Fisher exact test is 0.0572614. At a significance level of 0.10, we reject the null hypothesis. We have sufficient evidence to claim that there is a positive association between the treatment and the outcome.\n\n\n::: callout-important\nNote that the chi-squared assumption will yield a different decision for the hypothesis test, but this should be discarded because the chi-squared assumption does not hold (expected value less than 5).\n:::\n\n:::\n\n# Symmetry Tests\n\n## McNemar's Test\n\nMcNemar's test is a nonparametric test for homogeneity between two paired dichotomous variables.\n\n::: callout-note\nThe usual test of homogeneity assumes each cell count is independent, which would not be the case when the column and row variables are paired.\n\nThe McNemar's test is designed for a 2x2 contingency table. For larger nxn tables, we use the McNemar-Bowker Test.\n:::\n\n::: callout-tip\n## Example\n\nSuppose we are interested in the racial identity concordance of provider and patients at a certain region. The column variable could be assigned to describe the provider identity, while the row column could be assigned to describe the patient identity. The counts will not be independent of each other.\n:::\n\n## `R` implementation\n\nThe McNemar's test can be performed using `mcnemar.test()` in `R`. Like `chisq.test()` and `fisher.test()`, it can take individual columns or a contingency table as input.\n\n::: callout-note\nThe chi-squared distribution is used as the distribution of the test-statistic.\n:::\n\n## Example\n\nConsider the data in this table where patients were observed at Time 1 for the presence of a rash and then were observed at Time 2 for the presence of the rash. Each patient with a rash\nwas provided a homeopathic treatment between Time 1 and Time 2.\n\n::: panel-tabset\n\n### Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource_data<- data.frame(\n  T1_Presence= c(\"T1_Presence\", \"T1_Presence\", \"T1_Absence\", \"T1_Absence\"),\n  T2_Presence = c(\"T2_Presence\", \"T2_Absence\", \"T2_Presence\", \"T2_Absence\"),\n  Count = c(38, 12, 5, 45)\n)\n\n# Create a cross-table using xtabs\ncross_table_xtabs <- xtabs(Count ~ T1_Presence+T2_Presence, data = source_data)\ncross_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             T2_Presence\nT1_Presence   T2_Absence T2_Presence\n  T1_Absence          45           5\n  T1_Presence         12          38\n```\n\n\n:::\n:::\n\n\n\n### Question\n\nUse McNemar's test to see if there is a difference in proportion for those with the rash and without the rash before and\nafter treatment. Use a significance level of 0.05.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcn <- mcnemar.test(cross_table_xtabs)\nmcn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMcNemar's Chi-squared test with continuity correction\n\ndata:  cross_table_xtabs\nMcNemar's chi-squared = 2.1176, df = 1, p-value = 0.1456\n```\n\n\n:::\n:::\n\n\nThe resulting p-value is 0.1456101. At a significance level of 0.05, we fail to reject the null hypothesis. We have insufficient evidence to claim that there is a difference in proportion for those with the rash and without the rash before and\nafter treatment.\n:::\n\n\n# Measures of Association\n\n## Epidemiological Concepts\n\n::: callout-note\n## Observational Study\n\nA research method where researchers observe and collect information without manipulation.\n\n:::\n\n::: callout-note\n## Risk Factor\n\nA variable that is thought to be related to some outcome variable.\n\n:::\n\n## Prospective vs. Retrospective Study\n\n::: callout-note\n## Prospective Study\n\nIn a prospective study, two samples of subjects are selected: a group with the risk factor, and a group without the risk factor. These subjects are followed prospectively and outcomes are observed in the future.\n:::\n\n\n::: callout-note\n## Retrospective Study\n\nIn a retrospective study, two samples of subjects are selected: a group with the outcome of interest(cases), and a group without (control). These subjects are examined retrospectively to check if they have the risk factor.\n:::\n\n## Measures of Association\n\nFor prospective studies, the measure of association between the risk factor and outcome is the **relative risk**.\n\nFor retrospective studies, the measure of association between the risk factor and outcome is the **odds ratio**.\n\n## Relative Risk\n\n![](figs/10-contingency.png){width=5in}\n\nThe relative risk (RR) is the ratio of the risk of being a case among subjects with the risk factor to the risk of developing the disease among subjects without the risk factor. The RR can be calculated using the following equation:\n\n$$\nRR = \\frac{a/(a+b)}{c/(c+d)}\n$$\n\n## Odds Ratio\n\nRecall: Logistic Regression. The odds ratio is the ratio of the odds of developing into a case between those with and without the risk factor.\n\n$$\nOR = \\frac{a/b}{c/d}=ad/bc\n$$\n\n## `R` implementation\n\nWhile the best estimate of the odds ratios and relative risk can easily be calculated using the values in the contingency table, the package `epitools` contains the `epitab()` function that calculates both relative risk/odds ratio and their corresponding intervals.\n\n### Sample Code\n\n```{.r}\n#install.packages(\"epitools\")\nlibrary(epitools)\nepitab(contingency,method=\"oddsratio\")\n\n# OR\n\nepitab(contingency,method=\"riskratio\",pvalue=\"chi2\")\n\n```\n\n\n::: callout-important\nThe contingency table should be given in this particular format:\n\n```{.r}\n                 Disease\n  Exposure       No (ref)  Yes\n   Level 1 (ref)  a         b\n   Level 2        c         d\n   Level 3        e         f \n```\n\n:::\n\n## Example\n\nThe data below includes data from a prospective study on low-risk pregnant women. A group of 217 women did no voluntary or mandatory exercise during the pregnancy while 238 exercised extensively. One outcome variable of interest was experiencing preterm labor.\n\n::: panel-tabset\n\n### Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource_data<- data.frame(\n  Risk= c(\"Treatment\",\"Treatment\",\"Control\",\"Control\"),\n  Outcome = c(\"Case\", \"Control\", \"Case\", \"Control\"),\n  Count = c(22,216,18,199)\n)\n\n# Create a cross-table using xtabs\ncross_table_xtabs <- xtabs(Count ~ Risk+Outcome, data = source_data)\ncross_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Outcome\nRisk        Case Control\n  Control     18     199\n  Treatment   22     216\n```\n\n\n:::\n:::\n\n\n### Question\n\nCalculate the relevant measure of association and provide a 95% confidence interval.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(epitools)\nepitab(cross_table_xtabs,method=\"riskratio\",pvalue=\"chi2\",rev=\"columns\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$tab\n           Outcome\nRisk        Control        p0 Case         p1 riskratio     lower    upper\n  Control       199 0.9170507   18 0.08294931  1.000000        NA       NA\n  Treatment     216 0.9075630   22 0.09243697  1.114379 0.6145682 2.020672\n           Outcome\nRisk          p.value\n  Control          NA\n  Treatment 0.7211157\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"chi2\"\n```\n\n\n:::\n:::\n\n\nThe relative risk is 1.114 with a 95% confidence interval: (0.615,2.021). The resulting p-value from the chi-squared approximation is 0.72. At a significance level of 0.05, we fail to reject the null hypothesis. We have insufficient evidence to conclude that there is a difference in risk of experiencing preterm labor between those who exercise extensively and those who did not.\n\n:::\n\n## Exercise\n\nThe table shows 3970 subjects classified as cases or noncases of obesity and also classified according to smoking status of the mother during pregnancy (the risk factor).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource_data<- data.frame(\n  Risk= c(\"Present\",\"Present\",\"Absent\",\"Absent\"),\n  Outcome = c(\"Case\", \"Control\", \"Case\", \"Control\"),\n  Count = c(64,342,68,3496)\n)\n\n# Create a cross-table using xtabs\ncross_table_xtabs <- xtabs(Count ~ Risk+Outcome, data = source_data)\ncross_table_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Outcome\nRisk      Case Control\n  Absent    68    3496\n  Present   64     342\n```\n\n\n:::\n:::\n\n\n### Question\n\n- Is this study a prospective or a retrospective study?\n- Calculate the appropriate measure of association to report for this study. Provide a 95% confidence interval.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepitab(cross_table_xtabs,method=\"oddsratio\",rev=\"columns\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$tab\n         Outcome\nRisk      Control         p0 Case        p1 oddsratio    lower    upper\n  Absent     3496 0.91089109   68 0.5151515  1.000000       NA       NA\n  Present     342 0.08910891   64 0.4848485  9.620915 6.719328 13.77549\n         Outcome\nRisk           p.value\n  Absent            NA\n  Present 2.709464e-30\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n```\n\n\n:::\n:::\n\n\nThis is a retrospective study. The appropriate measure of association is the odds ratio. The estimated odds ratio is 9.62, with a 95% confidence interval: (6.72,13.78). The confidence interval shows that children with obesity are more likely to have had mothers who smoked than those without obesity.\n",
    "supporting": [
      "lecture10_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}