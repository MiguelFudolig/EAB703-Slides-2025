{
  "hash": "fcb12a0ec8020bc275995d2085662a54",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression Analysis\"\nsubtitle: Lecture 9\nformat:\n  revealjs:\n    theme: clean.scss\n    scrollable: true\n    footer: \"Lecture 9 - [Back to home](https://madfudolig.quarto.pub/introtobiostats/)\"\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    slide-number: true\n    menu: true\n    code-annotations: hover\n    chalkboard: true\n    engine: knitr\n    echo: true\n    code-fold: false\n    \n  pdf: \n    number-sections: true\n    engine: knitr\nbibliography: references.bib\n---\n\n# Outline\n\n-   Definition of Terms\n-   Simple Linear Regression\n-   Multiple Linear Regression\n-   Diagnostics\n-   Correlation\n\n# Definition of Terms\n\n## Prediction vs. Explanation\n\n::: callout-note\nIn a prediction problem, the investigators do not necessarily need to know the rule used to predict the outcome. Attention is focused on accuracy.\n\nIn an explanation problem, the investigators will often assume a functional form for the dependent variable, similar to the [linear model](https://madfudolig.quarto.pub/introtobiostats/lecture8.html#/general-form-of-linear-models) in Lecture 8.\n:::\n\n::: callout-important\nPrediction problems do not need a functional form, hence functional relations between independent and dependent variables will not be identified.\n:::\n\n## Correlation vs. Regression\n\n::: {.callout-note title=\"Regression Analysis\"}\nRegression analysis is helpful in assessing specific forms of the relationship between variables, and the ultimate objective when this method of analysis is employed usually is to predict or estimate the value of one variable corresponding to a given value of another variable.\n:::\n\n::: {.callout-note title=\"Correlation Analysis\"}\nCorrelation analysis is concerned with measuring the strength of the relationship between variables. It cannot predict or estimate unlike regression.\n:::\n\n# Simple Linear Regression\n\n## Linear Regression\n\nWe are often interested in how one or more predictor/independent variables are associated with an outcome/response/dependent variable. In linear regression, we assume this association is defined by a linear function.\n\n::: callout-important\nLinear regression models are used when the independent and dependent variables are both **continuous**.\n:::\n\n::: callout-important\nIt is important to keep in mind that our linear assumption is rarely met in practice. We may never know the true model, but information obtained from the linear model can still yield useful results, especially with exploratory data analysis.\n:::\n\n## Simple Linear Regression Model\n\nRecall Lecture 8: Linear regression models are a part of general linear models. In linear regression, a linear relationship is assumed between the independent variable $X$ and the dependent variable $Y$. For the $i^{th}$ observation ($X_i$,$Y_i$)\n\n$$\nY_i = \\beta_0 + \\beta_1X_i+ \\varepsilon_i\n$$\n\n::: callout-important\nThe error term $\\varepsilon_i$ are independent and identically distributed Gaussian errors (mean 0 and variance $\\sigma^2$). Recall that the error term contributes solely to the variance and not the mean of $Y$.\n:::\n\n## Example\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Method of Least Squares\n\nTo define the linear regression model, we need to estimate the following terms: the intercept, $\\beta_0$; the slope $\\beta_1$, and the variance $\\sigma^2$. The intercept and the slope can be estimated using the **method of least squares**.\n\n::: callout-note\nThe method of least squares can be visualized using the following [link](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm).\n:::\n\n## Estimation\n\nSuppose we have an estimate of the intercept $\\hat{\\beta}_0$ and the slope $\\hat{\\beta}_1$. The estimated value of the dependent variable $\\hat{Y}_i$ for a value of the independent variable $X_i$ can be expressed as:\n\n$$\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n$$\n\n## Assumptions of Linear Regression\n\n-   The means of the subpopulations of $Y$ all lie on the same straight line, also known as the linearity assumption.\n-   The values of $Y$ are statistically independent.\n-   For each value of $X$ there is a subpopulation of $Y$ values. For the usual inferential procedures of estimation and hypothesis testing to be valid, these subpopulations must be normally distributed.\n-   The variances of the subpopulations of $Y$ are all equal to $\\sigma^2$.\n-   The independent variable can be measured without error or with negligible error.\n\n::: callout-important\nThe first four assumptions are also known as the LINE assumptions: **Linearity, Independence, Normality, Equal Variances**\n:::\n\n## Hypothesis Testing: Linear Regression\n\nIn examining the association between $X$ and $Y$, the important parameter for inference is the slope $\\beta_1$.\n\n:::: panel-tabset\n### Slope and Association\n\n::: callout-important\nWhen $\\beta_1=0$, the best fit line resembles a horizontal line, indicating a lack of association between $X$ and $Y$.\n\nIf $\\beta_1 > 0$, the slope of the best fit line is positive, indicating a positive direct association between $X$ and $Y$.\n\nIf $\\beta_1<0$, the slope of the best fit line is negative, indicating a negative direct association between $X$ and $Y$.\n\nSimply put, $\\beta_1 \\neq 0$ indicates that there is an association between $X$ and $Y$.\n:::\n\n### Statistical Hypotheses\n\nThe null hypothesis of no association can be expressed as $H_0: \\beta_1=0$. The alternatives can be expressed as one-sided or two-sided.\n\n-   One-sided: $\\beta_1 >0$ or $\\beta_1 < 0$\n-   Two-sided: $\\beta_1 \\neq 0$\n::::\n\n## Test Statistic: $t$\n\nSuppose the estimated value of $\\beta_1$ is $\\hat{\\beta}_1$. The test statistic when testing the hypotheses for $\\beta_1$ can be expressed as:\n\n$$\nt = \\frac{\\hat{\\beta}_1}{\\sqrt{S^2/S_{XX}^2}}\n$$\n\n::: callout-note\n$S^2$ is given by the following equation:\n\n$$\nS^2 = \\frac{\\sum_i (Y_i-\\hat{Y}_i)^2}{(n-2)}\n$$\n\nThe term $(Y_i-\\hat{Y}_i)$ is also called the *residual* of the model. On the other hand, $S_{XX}^2$ is defined as the sum of squares of the independent variable.\n\n$$\nS_{XX}^2 = \\sum_i (X_i-\\bar{X})^2\n$$\n:::\n\n## Test Statistic: $F$\n\nAn analysis of variance (ANOVA) table can also be constructed from the regression model. This framework separates the source of variation accounted by the model and the error.\n\n| Source of Variation | Sum of Squares (SS) | Degrees of Freedom (DF) | Mean Squares (MS) | Variance Ratio (F) |\n|----|----|----|----|----|\n| Regression | SSR | 1 | MSR = SSR | MSR/MSE |\n| Error | SSE | n-2 | MSE = SSE/(n-2) |  |\n| Total | SST | n-1 |  |  |\n\nThe test statistic $F = MSR/MSE$ follows an F-distribution with degrees of freedom $(df_1,df_2) = (1,n-2)$.\n\n\n::: callout-note\n\nThis tests the following hypotheses: $H_0: \\beta_1=0$ and $H_a: \\beta_1 \\neq 0$.\n:::\n\n## `R` Implementation: `lm()`\n\nThe function `lm()` can perform calculations for linear regression. The function `summary()` is used with `lm()` to provide estimates for the intercept, slope, and residual variance. The `summary()` output also includes the results of the tests involving the slopes (both $t$ and $F$).\n\nSample code should follow how we used `lm()` in Lecture 8:\n\n```{.r}\nmod1 <- lm(DepVar~IndepVar,data=df)\nsummary(mod1)\n```\n\n\n\n\n## `R` implementation: Visualization\n\nThe `ggplot2` package is primarily used for data visualization. This package is included in the `tidyverse` package. \n\n::: callout-note\nThe `geom_point()` is used to plot the observed data points, while `geom_smooth(method,formula)` is used to overlay the best fit line on the observed data points.\n:::\n\n\n\n### Sample code \n\n```{.r}\nggplot(data=df,aes(x=IndepVar,y=DepVar)) +\ngeom_point() + \ngeom_smooth(method=\"lm\", formula=y~x)\n```\n\n\n## Framework for Analysis\n\nHere are the recommended steps in performing regression analysis:\n\n- Visualize the data using `ggplot2()`. Is there a discernible linear trend?\n- Obtain the equation of the best fit line using `lm()` and `summary()`.\n- Evaluate the equation to obtain a measure of the strength of association between the indepedent and dependent variables (hypothesis tests).\n- Use `geom_smooth()` as a litmus test of model fit.\n\n## Example\n\nThe data set `trees` (loaded in `R`) includes the measurements of the diameter (inches), height (ft), and volume (cubic ft) of timber in 31 felled black cherry trees. The diameter, labeled as Girth, is measured at 4'6\" above the ground.\n\n::: panel-tabset\n### Question\n\nTest the hypothesis that there is an association between the diameter and height of the cherry trees. Use simple linear regression to estimate the best fit line that estimates the height of the cherry trees based on its diameter.\n\n### Plot before Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data=trees,aes(x=Girth,y=Height)) + \n  geom_point()  + \n  theme_bw() + \n  labs(x=\"Diameter (in)\", y=\"Height (ft)\")\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Height~Girth,data=trees)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Height ~ Girth, data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5816  -2.7686   0.3163   2.4728   9.9456 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  62.0313     4.3833  14.152 1.49e-14 ***\nGirth         1.0544     0.3222   3.272  0.00276 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.538 on 29 degrees of freedom\nMultiple R-squared:  0.2697,\tAdjusted R-squared:  0.2445 \nF-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758\n```\n\n\n:::\n:::\n\n\n\nThe estimated equation for the tree height is $\\hat{height} = 1.0544*Diameter + 62.0313$.\n\nThe p-value of the coefficient is 0.0028, and the test statistic $t$ is 3.27 There is sufficient evidence of an association between diameter and height.\n\n### Plot after Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data=trees,aes(x=Girth,y=Height)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x,se=F) + \n  theme_bw() + \n  labs(x=\"Diameter (in)\", y=\"Height (ft)\")\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## `R` implementation: Estimation/Prediction\n\nRecall that we can estimate the value  of the dependent variable using our best fit line. Recall that for a value of the independent variable $X$, the estimated value of the dependent variable $\\hat{Y}$ is:\n\n$$\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 * X \n$$\n\nIn `R`, we can use the `predict` function with `lm()` to calculate predicted values for one or more values of the independent variable.\n\n::: panel-tabset\n\n### Extrapolation\n\n::: callout-warning\nEstimation/Prediction using linear regression works best if the considered value of the independent variable is within the range of the data used to fit the regression line (interpolation). Estimation outside this range is referred to as extrapolation, which is not a recommended practice.\n:::\n\n### Intercept\n\nThe intercept is the estimated value of the dependent variable when the independent variable is equal to 0. Interpreting the intercept can be considered as extrapolation if 0 is not within the range of the independent variable. It might also be trivial to interpret the intercept if the case of $X=0$ is trivial.\n\n### Sample Code\n\n```{.r}\nmod1 <- lm(DepVar~IndepVar,data=df)\nnewdata <- data.frame(IndepVar=c(X)) # substitute X with the value of the independent variable you want to estimate the value of the predictor for.\n\nnewdata$prediction <- predict(mod1,newdata)\nnewdata\n```\n\n:::\n\n## Example\n\nThe data set `trees` (loaded in `R`) includes the measurements of the diameter, height, and volume of timber in 31 felled black cherry trees. The diameter, labeled as Girth, is measured at 4'6\" above the ground.\n\n::: panel-tabset\n\n### Question\n\n- Calculate the estimated height of the black cherry trees with the following diameters: 10 inches, 15 inches, 20 inches.\n- Is it advisable to interpret the intercept? Explain.\n- Is it advisable to use this regression line to estimate the height of a black cherry tree with a 25-inch diameter? Explain.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Height~Girth,data=trees)\nnewdata <- data.frame(Girth=c(10,15,20))\nnewdata$prediction <- predict(mod1,newdata=newdata)\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Girth prediction\n1    10   72.57500\n2    15   77.84685\n3    20   83.11869\n```\n\n\n:::\n:::\n\nIt is not advisable to interpret the intercept because it is trivial to examine the case when the tree diameter is equal to zero. It is also outside the range of diameters considered in the study. Specifically, the range shown below does not include zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(trees$Girth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  8.3 20.6\n```\n\n\n:::\n:::\n\n\nSimilarly, X=25 is not in the range of diameters considered in the study and would lead to extrapolation if used in estimation. Hence, estimating the height of a tree with a 25-inch diameter using our best fit line is not recommended.\n:::\n",
    "supporting": [
      "lecture9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}