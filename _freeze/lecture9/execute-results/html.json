{
  "hash": "94661a2723d3dfc6969757a29bc9897d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression Analysis\"\nsubtitle: Lecture 9\nformat:\n  revealjs:\n    theme: clean.scss\n    scrollable: true\n    footer: \"Lecture 9 - [Back to home](https://madfudolig.quarto.pub/introtobiostats/)\"\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    slide-number: true\n    menu: true\n    code-annotations: hover\n    chalkboard: true\n    engine: knitr\n    echo: true\n    code-fold: false\n    \n  pdf: \n    number-sections: true\n    geometry:\n      - landscape\n      - margin=1in\n    include-in-header:\n      text: |\n        \\usepackage{titlesec}\n        \\titleformat{\\section}{\\normalfont\\large\\bfseries}{\\thesection}{1em}{}\n        \\titleformat{\\subsection}{\\normalfont\\large\\bfseries}{\\thesubsection}{1em}{}\n        \\titleformat{\\subsubsection}{\\normalfont\\normalsize\\bfseries}{\\thesubsubsection}{1em}{}\n        \n        % Syntax: \\titlespacing{command}{left-sep}{before-sep}{after-sep}\n        \\titlespacing{\\section}{0pt}{12pt}{12pt}\n        % \\titlespacing{\\subsection}{0pt}{3in}{12pt}\n        \\titlespacing{\\subsubsection}{0pt}{3in}{10pt}\n        \n        % This forces a clearpage before every subsection\n        \\let\\oldsubsection\\subsection\n        \\renewcommand{\\subsection}{\\clearpage\\oldsubsection}\n        \nbibliography: references.bib\n---\n\n# Outline\n\n-   Definition of Terms\n-   Simple Linear Regression\n-   Multiple Linear Regression\n-   Diagnostics\n-   Correlation\n\n# Definition of Terms\n\n## Prediction vs. Explanation\n\n::: callout-note\nIn a prediction problem, the investigators do not necessarily need to know the rule used to predict the outcome. Attention is focused on accuracy.\n\nIn an explanation problem, the investigators will often assume a functional form for the dependent variable, similar to the [linear model](https://madfudolig.quarto.pub/introtobiostats/lecture8.html#/general-form-of-linear-models) in Lecture 8.\n:::\n\n::: callout-important\nPrediction problems do not need a functional form, hence functional relations between independent and dependent variables will not be identified.\n:::\n\n## Correlation vs. Regression\n\n::: {.callout-note title=\"Regression Analysis\"}\nRegression analysis is helpful in assessing specific forms of the relationship between variables, and the ultimate objective when this method of analysis is employed usually is to predict or estimate the value of one variable corresponding to a given value of another variable.\n:::\n\n::: {.callout-note title=\"Correlation Analysis\"}\nCorrelation analysis is concerned with measuring the strength of the relationship between variables. It cannot predict or estimate unlike regression.\n:::\n\n# Simple Linear Regression\n\n## Linear Regression\n\nWe are often interested in how one or more predictor/independent variables are associated with an outcome/response/dependent variable. In linear regression, we assume this association is defined by a linear function.\n\n::: callout-important\nLinear regression models are used when the independent and dependent variables are both **continuous**.\n:::\n\n::: callout-important\nIt is important to keep in mind that our linear assumption is rarely met in practice. We may never know the true model, but information obtained from the linear model can still yield useful results, especially with exploratory data analysis.\n:::\n\n## Simple Linear Regression Model\n\nRecall Lecture 8: Linear regression models are a part of general linear models. In linear regression, a linear relationship is assumed between the independent variable $X$ and the dependent variable $Y$. For the $i^{th}$ observation ($X_i$,$Y_i$)\n\n$$\nY_i = \\beta_0 + \\beta_1X_i+ \\varepsilon_i\n$$\n\n::: callout-important\nThe error term $\\varepsilon_i$ are independent and identically distributed Gaussian errors (mean 0 and variance $\\sigma^2$). Recall that the error term contributes solely to the variance and not the mean of $Y$.\n:::\n\n## Example\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Method of Least Squares\n\nTo define the linear regression model, we need to estimate the following terms: the intercept, $\\beta_0$; the slope $\\beta_1$, and the variance $\\sigma^2$. The intercept and the slope can be estimated using the **method of least squares**.\n\n::: callout-note\nThe method of least squares can be visualized using the following [link](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm).\n:::\n\n## Estimation\n\nSuppose we have an estimate of the intercept $\\hat{\\beta}_0$ and the slope $\\hat{\\beta}_1$. The estimated value of the dependent variable $\\hat{Y}_i$ for a value of the independent variable $X_i$ can be expressed as:\n\n$$\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n$$\n\n::: callout-important\nThe slope estimate $\\hat{\\beta}_1$ is the expected change in the dependent variable for a 1-unit increase in $X_i$. Generally, for a $d$ unit increase in $X_i$, the dependent variable is expected to change by $d\\hat{\\beta}_1$.\n:::\n\n\n## Assumptions of Linear Regression\n\n-   The means of the subpopulations of $Y$ all lie on the same straight line, also known as the linearity assumption.\n-   The values of $Y$ are statistically independent.\n-   For each value of $X$ there is a subpopulation of $Y$ values. For the usual inferential procedures of estimation and hypothesis testing to be valid, these subpopulations must be normally distributed.\n-   The variances of the subpopulations of $Y$ are all equal to $\\sigma^2$.\n-   The independent variable can be measured without error or with negligible error.\n\n::: callout-important\nThe first four assumptions are also known as the LINE assumptions: **Linearity, Independence, Normality, Equal Variances**\n:::\n\n## Hypothesis Testing: Linear Regression\n\nIn examining the association between $X$ and $Y$, the important parameter for inference is the slope $\\beta_1$.\n\n:::: panel-tabset\n### Slope and Association\n\n::: callout-important\nWhen $\\beta_1=0$, the best fit line resembles a horizontal line, indicating a lack of association between $X$ and $Y$.\n\nIf $\\beta_1 > 0$, the slope of the best fit line is positive, indicating a positive direct association between $X$ and $Y$.\n\nIf $\\beta_1<0$, the slope of the best fit line is negative, indicating a negative direct association between $X$ and $Y$.\n\nSimply put, $\\beta_1 \\neq 0$ indicates that there is an association between $X$ and $Y$.\n:::\n\n### Statistical Hypotheses\n\nThe null hypothesis of no association can be expressed as $H_0: \\beta_1=0$. The alternatives can be expressed as one-sided or two-sided.\n\n-   One-sided: $\\beta_1 >0$ or $\\beta_1 < 0$\n-   Two-sided: $\\beta_1 \\neq 0$\n::::\n\n## Test Statistic: $t$\n\nSuppose the estimated value of $\\beta_1$ is $\\hat{\\beta}_1$. The test statistic when testing the hypotheses for $\\beta_1$ can be expressed as:\n\n$$\nt = \\frac{\\hat{\\beta}_1}{\\sqrt{S^2/S_{XX}^2}}\n$$\n\n::: callout-note\n$S^2$ is given by the following equation:\n\n$$\nS^2 = \\frac{\\sum_i (Y_i-\\hat{Y}_i)^2}{(n-2)}\n$$\n\nThe term $(Y_i-\\hat{Y}_i)$ is also called the *residual* of the model. On the other hand, $S_{XX}^2$ is defined as the sum of squares of the independent variable.\n\n$$\nS_{XX}^2 = \\sum_i (X_i-\\bar{X})^2\n$$\n:::\n\n## Test Statistic: $F$\n\nAn analysis of variance (ANOVA) table can also be constructed from the regression model. This framework separates the source of variation accounted by the model and the error.\n\n| Source of Variation | Sum of Squares (SS) | Degrees of Freedom (DF) | Mean Squares (MS) | Variance Ratio (F) |\n|----|----|----|----|----|\n| Regression | SSR | 1 | MSR = SSR | MSR/MSE |\n| Error | SSE | n-2 | MSE = SSE/(n-2) |  |\n| Total | SST | n-1 |  |  |\n\nThe test statistic $F = MSR/MSE$ follows an F-distribution with degrees of freedom $(df_1,df_2) = (1,n-2)$.\n\n\n::: callout-note\n\nThis tests the following hypotheses: $H_0: \\beta_1=0$ and $H_a: \\beta_1 \\neq 0$.\n:::\n\n## Goodness of Fit: $R^2$\n\nWe define the coefficient of determination, also known as $R^2$, as the following ratio:\n\n$$\nR^2 = SSR/(SSR + SSE)\n$$\n\n::: callout-important\n$R^2$ is non-negative and cannot exceed 1. The $R^2$ measures the proportion of the variability explained by the predictor in the model. The higher the value of $R^2$, the closer the fit of the data to the model.\n\n$R^2 = 1$ indicates a perfect fit, but is almost always not observed in real-life situations.\n:::\n\n\n## `R` Implementation: `lm()`\n\nThe function `lm()` can perform calculations for linear regression. The function `summary()` is used with `lm()` to provide estimates for the intercept, slope, and residual variance. The `summary()` output also includes the results of the tests involving the slopes (both $t$ and $F$).\n\nSample code should follow how we used `lm()` in Lecture 8:\n\n```{.r}\nmod1 <- lm(DepVar~IndepVar,data=df)\nsummary(mod1)\n```\n\n::: callout-tip\nThe `summary()` function also provides the $R^2$ value of the model.\n:::\n\n\n## `R` implementation: Visualization\n\nThe `ggplot2` package is primarily used for data visualization. This package is included in the `tidyverse` package. \n\n::: callout-note\nThe `geom_point()` is used to plot the observed data points, while `geom_smooth(method,formula)` is used to overlay the best fit line on the observed data points.\n:::\n\n\n\n### Sample code \n\n```{.r}\nggplot(data=df,aes(x=IndepVar,y=DepVar)) +\ngeom_point() + \ngeom_smooth(method=\"lm\", formula=y~x)\n```\n\n\n## Framework for Analysis\n\nHere are the recommended steps in performing regression analysis:\n\n- Visualize the data using `ggplot2()`. Is there a discernible linear trend?\n- Obtain the equation of the best fit line using `lm()` and `summary()`.\n- Evaluate the equation to obtain a measure of the strength of association between the independent and dependent variables (hypothesis tests).\n- Use `geom_smooth()` as a litmus test of model fit.\n\n## Example\n\nThe data set `trees` (loaded in `R`) includes the measurements of the diameter (inches), height (ft), and volume (cubic ft) of timber in 31 felled black cherry trees. The diameter, labeled as Girth, is measured at 4'6\" above the ground.\n\n::: panel-tabset\n### Question\n\nTest the hypothesis that there is an association between the diameter and height of the cherry trees. Use simple linear regression to estimate the best fit line that estimates the height of the cherry trees based on its diameter. What is the $R^2$ value?\n\n### Plot before Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data=trees,aes(x=Girth,y=Height)) + \n  geom_point()  + \n  theme_bw() + \n  labs(x=\"Diameter (in)\", y=\"Height (ft)\")\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Height~Girth,data=trees)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Height ~ Girth, data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5816  -2.7686   0.3163   2.4728   9.9456 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  62.0313     4.3833  14.152 1.49e-14 ***\nGirth         1.0544     0.3222   3.272  0.00276 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.538 on 29 degrees of freedom\nMultiple R-squared:  0.2697,\tAdjusted R-squared:  0.2445 \nF-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758\n```\n\n\n:::\n:::\n\n\n\nThe estimated equation for the tree height is $\\hat{height} = 1.0544*Diameter + 62.0313$.\n\nThe p-value of the coefficient is 0.0028, and the test statistic $t$ is 3.27. There is sufficient evidence of an association between diameter and height. The value of $R^2$ is 0.27.\n\n### Plot after Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data=trees,aes(x=Girth,y=Height)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x,se=F) + \n  theme_bw() + \n  labs(x=\"Diameter (in)\", y=\"Height (ft)\")\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## `R` implementation: Estimation/Prediction\n\nRecall that we can estimate the value  of the dependent variable using our best fit line. Recall that for a value of the independent variable $X$, the estimated value of the dependent variable $\\hat{Y}$ is:\n\n$$\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 * X \n$$\n\nIn `R`, we can use the `predict` function with `lm()` to calculate predicted values for one or more values of the independent variable. It can also output confidence and prediction intervals\n\n::: panel-tabset\n\n### Extrapolation\n\n::: callout-warning\nEstimation/Prediction using linear regression works best if the considered value of the independent variable is within the range of the data used to fit the regression line (interpolation). Estimation outside this range is referred to as extrapolation, which is not a recommended practice.\n:::\n\n### Intercept\n\nThe intercept is the estimated value of the dependent variable when the independent variable is equal to 0. Interpreting the intercept can be considered as extrapolation if 0 is not within the range of the independent variable. It might also be trivial to interpret the intercept if the case of $X=0$ is trivial.\n\n### Confidence vs. Prediction Intervals\n\n**Confidence intervals** are made for parameters, whereas an interval for random variables are called  **prediction intervals**. Prediction interval are generally wider than confidence intervals.\n\n::: callout-important\nFor example, if we want to determine an interval estimate for the mean height of a tree with girth $X$, then we use a confidence interval. If we want to estimate the height of a particular tree with girth $X$, then we use a prediction interval\n:::\n\n### Sample Code\n\n```{.r}\nmod1 <- lm(DepVar~IndepVar,data=df)\nnewdata <- data.frame(IndepVar=c(X)) # substitute X with the value of the independent variable you want to estimate the value of the predictor for.\n\nnewdata$prediction <- predict(mod1,newdata,interval = \"confidence\",level = 0.95)\nnewdata$prediction <- predict(mod1,newdata,interval = \"prediction\",level = 0.95)\nnewdata\n```\n\n:::\n\n## Example\n\nThe data set `trees` (loaded in `R`) includes the measurements of the diameter, height, and volume of timber in 31 felled black cherry trees. The diameter, labeled as Girth, is measured at 4'6\" above the ground.\n\n::: panel-tabset\n\n### Question\n\n- Calculate the estimated height of the black cherry trees with the following diameters: 10 inches, 15 inches, 20 inches with the corresponding prediction intervals.\n- Is it advisable to interpret the intercept? Explain.\n- Is it advisable to use this regression line to estimate the height of a black cherry tree with a 25-inch diameter? Explain.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Height~Girth,data=trees)\nnewdata <- data.frame(Girth=c(10,15,20))\nnewdata$prediction <- predict(mod1,newdata=newdata,interval = \"prediction\",level = 0.95)\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Girth prediction.fit prediction.lwr prediction.upr\n1    10       72.57500       60.86890       84.28110\n2    15       77.84685       66.28041       89.41328\n3    20       83.11869       70.77983       95.45755\n```\n\n\n:::\n:::\n\nIt is not advisable to interpret the intercept because it is trivial to examine the case when the tree diameter is equal to zero. It is also outside the range of diameters considered in the study. Specifically, the range shown below does not include zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(trees$Girth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  8.3 20.6\n```\n\n\n:::\n:::\n\n\nSimilarly, X=25 is not in the range of diameters considered in the study and would lead to extrapolation if used in estimation. Hence, estimating the height of a tree with a 25-inch diameter using our best fit line is not recommended.\n:::\n\n## Exercise\n\nConsider the liver steatosis data set `Liver_Steatosis.csv`. This dataset contains information on the 443 of 451 patients who had bariatric surgery at the Cleveland Clinic between 2005 and 2009 and underwent livery biopsy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nliver <- read.csv(\"Liver_Steatosis.csv\")\n```\n:::\n\n\n\n::: panel-tabset\n\n\n\n### Question\n\nSuppose we want to investigate whether we can use an individual's weight in kg (variable weight) to estimate their LDL levels (variable LDL). \n\n- Estimate the best fit line using regression methods.\n- Test for an association between weight and LDL values.\n- Predict the LDL value for an individual weighing 120 kg. Provide a 95% prediction interval.\n- Provide the value of $R^2$.\n- Create a scatter plot with the regression line overlayed using `ggplot`.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=liver,aes(x=Weight,y=LDL))+\n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x,se=F) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\nThere appears to be a weak association between LDL and weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(LDL~Weight,data=liver)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = LDL ~ Weight, data = liver)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.282 -22.991  -0.458  19.363 138.310 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 108.89842    7.68701  14.167   <2e-16 ***\nWeight       -0.02319    0.05618  -0.413     0.68    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33 on 393 degrees of freedom\n  (48 observations deleted due to missingness)\nMultiple R-squared:  0.0004335,\tAdjusted R-squared:  -0.00211 \nF-statistic: 0.1704 on 1 and 393 DF,  p-value: 0.68\n```\n\n\n:::\n:::\n\n\nThe best fit line is $\\hat{LDL} = 108.90 - 0.02*Weight$. The p-value of the coefficient is 0.68, and the test statistic $t$ is -0.41. There is no evidence of an association between weight and LDL levels. The $R^2$ value is 4\\times 10^{-4}. Note that **there are 48 observations deleted due to missingness**. These variables were excluded from the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(Weight=c(120))\nnewdata$prediction <- predict(mod1,newdata=newdata,interval = \"prediction\",level = 0.95)\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Weight prediction.fit prediction.lwr prediction.upr\n1    120      106.11543       41.13065      171.10020\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n# Multiple Linear Regression\n\n## Multiple Linear Regression\n\nOften, we want to consider the effect of multiple predictors on the dependent variable. The simple linear regression can be extended to any number of predictor variables. We define $X_{ij}$ as the value of predictor $j$ on unit $i$.\n\nThe multiple linear regression model extended to $p$ predictors is then:\n\n$$\nY_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... + \\beta_pX_{ip} + \\varepsilon_i\n$$\n\nThe estimated best fit line can be written as:\n\n$$\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_{1i} + \\hat{\\beta}_2X_{2i} + ... + \\hat{\\beta}_pX_{ip} \n$$\n\n::: callout-important\nThe slope estimate $\\hat{\\beta}_k$ is the expected change in the dependent variable for a 1-unit increase in $X_ki$ assuming all other variables remain constant. Generally, for a $d$ unit increase in $X_ki$, the dependent variable is expected to change by $d\\hat{\\beta}_1$.\n:::\n\n## Hypothesis Tests: Significance of Regression\n\nThe testing of significance of regression involves the null hypothesis $H_0: \\beta_1=\\beta_2=...=\\beta_p=0$, assuming that there is a total of $p$ predictors. The alternative hypothesis that at least one coefficient is non-zero. The test of significance of regression is used to test if there are any predictors with a non-zero coefficient. This test uses the F-statistic similar to what was discussed for simple linear regression.\n\n::: callout-note\n`R` provides this information in the `summary()` output of `lm()` with the F-statistic and the corresponding p-value. The corresponding degrees of freedom are (p,N-p-1).\n:::\n\n## Hypothesis Tests: Individual Coefficients\n\nWhen the null hypothesis of the significance of regression is rejected, we can perform hypothesis tests for each coefficient estimated.\n\n::: callout-note\n`R` provides this information in the `summary()` output of `lm()` with the t-statistic and the corresponding p-value. \n:::\n\n## Discrete Variables as Predictors\n\nIt is possible that we have a categorical independent variable that we want to consider for the regression model. \n\n::: panel-tabset\n### Dichotomous Variables\n\nFor a *dichotomous* variable, we can introduce a variable such that a \"Yes\" condition is assigned to `1` and `0` otherwise.\n\n### Multi-level\n\nFor a categorical predictor variable with $k$ levels, we can introduce a $k-1$ **dichotomous dummy variables** to describe each level of the categorical predictor variable. \n\n::: callout-note\n\nFor example, blood type (A, B, AB, O) has four levels. This means we need three dummy variables $(X_1,X_2,X_3)$ to account for blood type. We define these dummy variables as:\n\n$$\nX_1 = \n\\begin{cases}\n1, BloodType=A\\\\\n0, otherwise\n\\end{cases}\n$$\n\n$$\nX_2 = \n\\begin{cases}\n1, BloodType=B\\\\\n0, otherwise\n\\end{cases}\n$$\n\n$$\nX_3 = \n\\begin{cases}\n1, BloodType=AB\\\\\n0, otherwise\n\\end{cases}\n$$\n\nFor an individual with blood type $A$, the dummy variable values are $(X_1,X_2,X_3)=(1,0,0)$. Similarly, $B$ corresponds to $(X_1,X_2,X_3)=(0,1,0)$. For blood type O, $(X_1,X_2,X_3)=(0,0,0)$.\n:::\n\n### `R` Implementation\n\n`lm()` creates dummy variables for character/factor variables. If the variable as a numeric variable, we need to use `as.factor()` for `R` to recognize its categorical nature.\n:::\n\n## Adjusted $R^2$\n\nWhen we add more predictor variables to a model, the $R^2$ can only go up or stay the same. It is tempting to add more predictor variables to increase $R^2$ even if these variables have little to no effect.\n\n::: callout-tip\nThe adjusted $R^2$ value, also known as $R_{adj}^2$, is used to penalize models with more parameters. One form of the adjusted $R^2$ is given by:\n\n$$\nR_{adj}^2 = 1-\\frac{(N-1)}{(N-p-1)}(1-R^2)\n$$\n\nwhere $N$ is the total number of samples. This is also provided in the `summary()` output of `lm()`. \n\n:::\n\n## Model Checking\n\nRecall: The residual is defined as the difference between the observed and predicted responses, specifically $r_i = y_i-\\hat{y}_i$. The residuals can be used to test the normality, equal variance, and independence assumptions. What is of interest to us are the **residual plots**.\n\nThe following tabs explain the plots shown when plotting an `lm()` object using `plot()`.\n\n```{.r}\nmod1 <- lm(y~x1+x2,data=df)\nplot(mod1)\n```\n\n::: panel-tabset\n\n### Residual v. Fitted\n\nThis plot shows the fitted values of the predictor data to the residuals. If the results can be confined in a horizontal band, there are no obvious model defects.\n\n### Q-Q Residuals\n\nThe normal probability plot of residuals, also known as the QQ plot, plots the theoretical quantiles of the normal distribution to the residuals. A good plot shows the points coinciding with the dashed line, defined as the expected behavior of the residuals if they follow a normal distribution.\n\n::: callout-important\nSmall deviations from this line is not a major source of concern. Visual inspection of the QQ plot is more recommended than formal statistical tests for normality, as these residuals are not independent. The non-independence of the residuals breaks the independence assumptions of most normality tests.\n:::\n\n### Scale-Location\n\nThe Scale-Location plot is a plot of the fitted values against the square root of the standardized residuals. Ideally, the red line should be generally horizontal with no upward trend, sharp angles, or slope.\n\n### Residuals v. Leverage\n\nThe residuals vs. leverage plot provides information on highly influential points in the model. Highly influential data points might lead to a skewed model. If the points are within the dashed lines, then there are no highly influential points in the model.\n\n::: \n\n## Example\n\nConsider the sleep health data set in `SleepHealthData.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep <- read.csv(\"SleepHealthData.csv\")\n```\n:::\n\n\n::: panel-tabset\n### Question\n\nSuppose researchers are interested in estimating sleep duration based on heart rate, age, and gender.\n\n- Estimate the best fit line using multiple regression\n- Comment on the results of the test of significance of regression. Is there evidence of non-zero coefficients?\n- Comment on the results of the hypothesis tests for each coefficient. Which predictors do we have evidence of an association with sleep duration?\n- Provide the $R^2$ and the adjusted $R^2$ for this model.\n- Comment on the plots used for model checking. Are there major model defects?\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(sleep_duration~gender + age + heart_rate,data=sleep)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sleep_duration ~ gender + age + heart_rate, data = sleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0447 -0.6814  0.1215  0.5053  2.0566 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.084352   0.650015  18.591  < 2e-16 ***\ngenderMale   0.304680   0.083770   3.637 0.000315 ***\nage          0.032200   0.004845   6.646 1.08e-10 ***\nheart_rate  -0.092133   0.008356 -11.027  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6465 on 370 degrees of freedom\nMultiple R-squared:  0.345,\tAdjusted R-squared:  0.3397 \nF-statistic: 64.96 on 3 and 370 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe best fit line to estimate sleep duration is $\\hat{y} = 12.08 +  0.305*X_{male} +0.03*Age - 0.09*HeartRate$, where $X_{male}=1$ if the individual is male and 0 otherwise. \n\nThe overall test of significance of regression yielded an F-statistic of F(3,370) = 64.96 corresponding to a p-value <2.2e-16, indicating there is sufficient evidence that there is at least one non-zero coefficient in the assumed model. \n\nThe individual hypothesis tests show that there is evidence of associations between sleep duration and gender ($\\beta$ = 0.30, p=0.0003), age ($\\beta$=0.03, p=1.08e-10), and heart rate ($\\beta$=-0.09,p<2.2e-16).\n\nThe corresponding $R^2$ and adjusted $R^2$ values are 0.345 and 0.3397.\n\n### Plots\n\nThe plots show major defects on the normality assumption of the residuals, which we must keep in mind when we are interpreting our results. We can also see that the residuals vs. fitted values cannot be enclosed in two horizontal bands, implying evidence of unequal variances.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod1)\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-13-2.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-13-3.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-13-4.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## Exercise\n\nConsider the liver steatosis data set `Liver_Steatosis.csv`. This dataset contains information on the 443 of 451 patients who had bariatric surgery at the Cleveland Clinic between 2005 and 2009 and underwent livery biopsy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nliver <- read.csv(\"Liver_Steatosis.csv\")\n```\n:::\n\n\n::: panel-tabset\n### Question\n\nSuppose researchers are interested in estimating a person's total cholesterol levels (variable CHOL) based on their BMI (variable BMI), age (variable Age), history of metabolic syndrome (variable MET_Syndrome, equal to 1 means patient has history), and plasma triglycerides (variable TG).\n\n- Estimate the best fit line using multiple regression.\n- Comment on the results of the test of significance of regression. Is there evidence of non-zero coefficients?\n- Comment on the results of the hypothesis tests for each coefficient. Which predictors do we have evidence of an association with total cholesterol levels?\n- Provide the $R^2$ and the adjusted $R^2$ for this model.\n- Comment on the plots used for model checking. Are there major model defects?\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(CHOL~BMI+ Age + MET_Syndrome+TG,data=liver)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CHOL ~ BMI + Age + MET_Syndrome + TG, data = liver)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-142.490  -25.349   -1.504   22.242  134.882 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  152.18618   12.46599  12.208   <2e-16 ***\nBMI            0.09060    0.17538   0.517   0.6057    \nAge            0.19541    0.17158   1.139   0.2554    \nMET_Syndrome  -9.98380    4.36471  -2.287   0.0227 *  \nTG             0.15812    0.01478  10.699   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37.59 on 406 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.2233,\tAdjusted R-squared:  0.2157 \nF-statistic: 29.19 on 4 and 406 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\nThe best fit line to estimate total cholesterol levels is $\\hat{y} = 152.19 +  0.091*BMI + 0.195*Age -9.98*X_{MetSyndrome} + 0.158*TG$. where $X_{MetSyndrome}=1$ when patient has history of metabolic syndrome. \n\nThe overall test of significance of regression yielded an F-statistic of F(4,406) = 29.19 corresponding to a p-value <2.2e-16, indicating there is sufficient evidence that there is at least one non-zero coefficient in the assumed model. \n\nThe individual hypothesis tests show that there is evidence of associations between total cholesterol and Metabolic Syndrome history ($\\beta$ = -9.98, p=0.0003) and triglycerides ($\\beta$=0.158,p<2.2e-16). There is no evidence of association between total cholesterol and BMI ($\\beta$ = 0.09, p=0.61) and Age ($\\beta$ = 0.195, p=0.25).\n\nThe corresponding $R^2$ and adjusted $R^2$ values are 0.2233 and 0.2157.\n\n### Plots\n\nA sharp curve in the residuals vs. fitted plot could be indicative of unequal variances. Normality assumption appears to hold well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod1)\n```\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-16-2.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-16-3.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](lecture9_files/figure-revealjs/unnamed-chunk-16-4.png){width=960}\n:::\n:::\n\n\n\n:::\n\n# Correlation\n\n## Correlation\n\nIn regression analysis, we assumed that the predictor variables were constant and the only random variable was the response or outcome. Often, we encounter scenarios where both response and predictor variables are random. This is when we use a *correlation model*.\n\n::: callout-note\nIn the previous exercise, cholesterol and triglycerides can be treated as random if they are measured at the same time.\n:::\n\n::: callout-important\nAlthough correlation analysis cannot be carried out meaningfully under the classic regression model, regression analysis can be carried out under the correlation model.\n:::\n\n## Correlation Coefficient\n\nThe population correlation coefficient $\\rho$ measures the strength of linear relationship between the predictor $X$ and $Y$. On the other hand, The sample correlation coefficient, $r$, describes the linear relationship between the sample observations on two variables in the same way that $\\rho$ describes the relationship in a population. \n\n::: callout-important\n$\\rho$ and $r$ can take on values between -1 and 1. Negative values of the correlation coefficient indicate a negative/inverse association, while positive values of the correlation coefficient indicate a positive/direct association.\n\nA value of the correlation coefficient equal to 1 indicates perfect positive linear association, i.e. all data points fit in a line with a positive slope.\n\nA value of the correlation coefficient equal to -1 indicates perfect negative linear association, i.e. all data points fit in a line with a negative slope.\n\nA value of the correlation coefficient equal to 0 indicates no association.\n:::\n\n\n## Types of Correlation Coefficients\n\nThere are two main types of correlation coefficients: Pearson and Spearman correlation coefficients.\n\n::: callout-note\nPearson correlation coefficients measures the strength of **linear relationship**. This is typically used in most studies.\n\nSpearman correlation coefficients measures the strength of monotonic relationship using non-parametric methods. It is used to measure association that might not be linear, or in the presence of outliers.\n:::\n\nWe will focus on Pearson correlation coefficients in this chapter.\n\n## A Stern Warning\n\nAlways remember: **CORRELATION DOES NOT MEAN CAUSATION**. No matter how high the correlation coefficient values are, this is not evidence that $X$ causes $Y$ or vice versa.\n\nExamples of spurious correlation can be found [here](https://www.tylervigen.com/spurious-correlations)\n\n## Hypothesis Test: Correlation Coefficient $\\rho$\n\nWe can perform a test on the correlation coefficient $\\rho$ to determine if we have evidence of linear association between the two variables. We use the null hypothesis of no association $H_0: \\rho=0$. The alternative hypotheses can be one-sided ($\\rho < 0$ or $\\rho > 0$), or two-sided ($\\rho \\neq 0$).\n\nThe test statistic can be calculated from the sample correlation coefficient $r$.\n\n$$\nt = r \\sqrt{\\frac{(n-2)}{(1-r^2)}}\n$$\n\nThis test statistic follows a t-distribution with $n-2$ degrees of freedom, where $n$ is the total sample size.\n\n## `R` implementation\n\nThe function `cor.test(x,y)` can be used to estimate the sample correlation coefficient $r$, as well as perform the hypothesis test of no association and estimate confidence interval for $\\rho$.\n\n```{.r}\ncor.test(df$x,df$y,conf.level=0.95)\n\n# OR\n\ncor.test(~x+y, data=df,conf.level=0.95)\n```\n\n::: callout-important\nNote that the formula is slightly different from `lm()` and `aov()`.\n:::\n\n## Example\n\nConsider the sleep health data `SleepHealthData.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep <- read.csv(\"SleepHealthData.csv\")\n```\n:::\n\n\n\n::: panel-tabset\n### Question\n\nTest for an association between stress level and sleep duration using correlation analysis.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncortest <- cor.test(sleep$sleep_duration,sleep$stress_level,conf.level=0.95)\n\ncortest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  sleep$sleep_duration and sleep$stress_level\nt = -26.739, df = 372, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8430912 -0.7732074\nsample estimates:\n      cor \n-0.811023 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(~sleep_duration+stress_level,data=sleep,conf.level=0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  sleep_duration and stress_level\nt = -26.739, df = 372, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8430912 -0.7732074\nsample estimates:\n      cor \n-0.811023 \n```\n\n\n:::\n:::\n\n\nThe estimated correlation coefficient between sleep duration and stress level is -0.811 with a 95% confidence interval of  -0.843, -0.773. \n\nThe hypothesis test shows that the test statistic is -26.739 with 372 degrees of freedom. This corresponds to a p-value <2.2e-16, indicating we have sufficient evidence of a correlation between sleep duration and stress level.\n:::\n\n\n\n## Exercise\n\nConsider the liver steatosis data set `Liver_Steatosis.csv`. This dataset contains information on the 443 of 451 patients who had bariatric surgery at the Cleveland Clinic between 2005 and 2009 and underwent livery biopsy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nliver <- read.csv(\"Liver_Steatosis.csv\")\n```\n:::\n\n\n\n::: panel-tabset\n### Question\n\nTest for an association between BMI (variable BMI) and cholesterol (variable CHOL) using correlation analysis.\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncortest <- cor.test(liver$BMI,liver$CHOL,conf.level=0.95)\n\ncortest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  liver$BMI and liver$CHOL\nt = -0.13162, df = 409, p-value = 0.8954\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.10317221  0.09027825\nsample estimates:\n        cor \n-0.00650787 \n```\n\n\n:::\n:::\n\n\n\nThe estimated correlation coefficient between BMI and cholesterol is -0.007 with a 95% confidence interval of  -0.103, 0.09. \n\nThe hypothesis test shows that the test statistic is -0.132 with 409 degrees of freedom. This corresponds to a p-value of 0.8953525, indicating we have insufficient evidence of a correlation between BMI and cholesterol.\n:::\n\n\n\n",
    "supporting": [
      "lecture9_files\\figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}