{
  "hash": "e94f5b8cdc29be812486ac1f86daa3a3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability Distributions\"\nsubtitle: Lecture 4\nformat:\n  revealjs:\n    theme: clean.scss\n    scrollable: true\n    footer: \"Lecture 4 - [Back to home](https://madfudolig.quarto.pub/introtobiostats/)\"\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    slide-number: true\n    menu: true\n    code-annotations: hover\n    chalkboard: true\n    engine: knitr\n    echo: true\n    code-fold: false\n    \n  pdf: \n    number-sections: true\n    geometry:\n      - landscape\n      - margin=1in\n    include-in-header:\n      text: |\n        \\usepackage{titlesec}\n        \\titleformat{\\section}{\\normalfont\\large\\bfseries}{\\thesection}{1em}{}\n        \\titleformat{\\subsection}{\\normalfont\\large\\bfseries}{\\thesubsection}{1em}{}\n        \\titleformat{\\subsubsection}{\\normalfont\\normalsize\\bfseries}{\\thesubsubsection}{1em}{}\n        \n        % Syntax: \\titlespacing{command}{left-sep}{before-sep}{after-sep}\n        \\titlespacing{\\section}{0pt}{12pt}{12pt}\n        % \\titlespacing{\\subsection}{0pt}{3in}{12pt}\n        \\titlespacing{\\subsubsection}{0pt}{3in}{10pt}\n        \n        % This forces a clearpage before every subsection\n        \\let\\oldsubsection\\subsection\n        \\renewcommand{\\subsection}{\\clearpage\\oldsubsection}\n        \nbibliography: references.bib\n---\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n# Outline\n\n-   Probability Distributions of Discrete Random Variables\n\n    -   Binomial Distribution\n    -   Poisson Distribution\n    -   Other Distributions\n\n-   Probability Distributions of Continuous Random Variables\n\n    -   Uniform Distribution\n    -   Gaussian (Normal) Distribution\n    -   Other Distributions\n\n-   Applications of the Gaussian Distribution\n\n# Probability Distributions of Discrete Random Variables\n\n## Discrete Random Variables\n\n::: {.callout-note title=\"Discrete Random Variable\"}\nA random variable is discrete if it can only take on a finite or countably infinite number of values.\n:::\n\n::: callout-tip\nIntegers are countably infinite, hence counts are considered discrete.\n\nQualitative variables often take on a finite number of values.\n:::\n\n## Discrete Probability Distribution\n\nThe probability distribution of a discrete random variable is a **table, graph, formula, or other device** used to specify all possible values of a discrete random variable along with their respective probabilities.\n\n::: callout-tip\nFormulas that describe discrete probability distributions are also known as the probability mass function (pmf). The function is often denoted as $P(X=x)$, $p_X(x)$, or simply $p(x)$.\n:::\n\n::: callout-important\nNotation is important here. $X$ is the random variable, $x$ is the outcome/realization. $X=x$ is the event that the random variable is equal to the outcome $x$.\n:::\n\n## Example 1\n\nConsider a random variable $X$ that represents the outcome of a fair six-sided die. Prior to rolling the die, $X$ can take on any one of the six values: {1,2,3,4,5,6}. The discrete probability distribution can be expressed in the following forms:\n\n:::: panel-tabset\n### Table\n\n| Outcome of $X$ | p(x) |\n|----------------|------|\n| 1              | 1/6  |\n| 2              | 1/6  |\n| 3              | 1/6  |\n| 4              | 1/6  |\n| 5              | 1/6  |\n| 6              | 1/6  |\n\n### Formula\n\n$$\np(x) = \\begin{cases} \n      1/6,~ if~x=1,2,...,6 \\\\\n      0, ~ otherwise\n   \\end{cases}\n$$\n\n### Graph\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n\n### Notes\n\n::: callout-important\nHere, $X$ is the result of rolling the die, $x$ is the outcome measured after rolling the die which can be any of the following numbers: {1,2,3,4,5,6}.\n:::\n\nThis PMF is also referred to as a **uniform discrete distribution** because the probability is uniformly distributed across the sample space.\n::::\n\n## Example 2.1\n\nThe frequency table below shows the results of a survey in which participants residing in the Appalachian region of southern Ohio were asked how many food assistance programs they had used in the last 12 months.\n\n::: panel-tabset\n### Frequency Table\n\n| Number of Programs | Frequency |\n|--------------------|-----------|\n| 1                  | 62        |\n| 2                  | 47        |\n| 3                  | 39        |\n| 4                  | 39        |\n| 5                  | 58        |\n| 6                  | 37        |\n| 7                  | 4         |\n| 8                  | 11        |\n| Total              | 297       |\n\n### Probability Distribution\n\nWe can calculate the probability distribution by calculating the relative frequency of each outcome. This can be done by dividing the frequencies by the total (297).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(`Number of Programs`=factor(c(1:8), levels=c(1:8)),\nFrequency = c(62,47,39,39,58,37,4,11),\nRelative.Frequency = c(62,47,39,39,58,37,4,11)/297) \n\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Number.of.Programs Frequency Relative.Frequency\n1                  1        62         0.20875421\n2                  2        47         0.15824916\n3                  3        39         0.13131313\n4                  4        39         0.13131313\n5                  5        58         0.19528620\n6                  6        37         0.12457912\n7                  7         4         0.01346801\n8                  8        11         0.03703704\n```\n\n\n:::\n:::\n\n\n### Graph\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(df, aes(x=Number.of.Programs,y=Relative.Frequency)) + \n  geom_col(fill=\"gray\",color=\"black\") + # <1>\n  theme_bw() + # <2>\n  labs(x=\"Number of Programs\",y=\"p(x)\") # <3>\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n1.  Formatting the bar plot with a gray fill and black outline\n2.  `theme_bw` using a black and white theme, which looks good aesthetically\n3.  Axes labels were changed.\n:::\n\n## Example 2.2\n\nConsider the probability distribution in Example 2.1.\n\n-   What is the probability of randomly selecting a family who used four assistance programs?\n\n$p(4) = 0.1313$\n\n-   What is the probability of randomly selecting a family who used one or three assistance programs?\n\n::: callout-important\nThese events are disjoint, hence P(one AP AND three AP 0) = 0. Using the addition rule,\n\n$P(one~ \\cup~ three) = P(one) + P(three) =$ 0.34\n:::\n\n## PMF: Properties\n\nThe discrete probability distribution should satisfy the following properties:\n\n1.  $0 \\leq p(x) \\leq 1$ for all $x$.\n2.  $\\sum_{all~x} p(x) = 1$ (The sum of the probabilities of all disjoint outcomes in the sample space is $1$.)\n\n## Cumulative Distributions\n\nIn addition to discrete probability distributions, discrete variables also have cumulative distribution functions (CDF)\n\n::: callout-tip\nThe CDF for a discrete variable can be calculated by successively adding the probabilities of the outcome of interest and others before it.\n:::\n\n::: callout-note\nThe CDF is often referred to as $F_X(x)$ or $F(x)$ to signify $P(X \\leq x)$.\n:::\n\n## Example 1\n\nConsider the PMF of the fair six-sided die. The cumulative distribution can be calculated using the cumulative relative frequency.\n\n| Outcome of $X$ | p(x) | F(x) = P(X$\\leq$x) |\n|----------------|------|-----------------------|\n| 1              | 1/6  | 1/6                   |\n| 2              | 1/6  | 2/6                   |\n| 3              | 1/6  | 3/6                   |\n| 4              | 1/6  | 4/6                   |\n| 5              | 1/6  | 5/6                   |\n| 6              | 1/6  | 6/6                   |\n\n## Example 2\n\nConsider the PMF for the food assistance example. We can calculate and visualize the cumulative relative frequency using `R`.\n\n::: panel-tabset\n\n### Table\nTo create a separate table as a function of existing tables, we use the `mutate(data_frame,expr)` function in the `dplyr` package available in `tidyverse`. The cumulative sum can also be calculated automatically using the `cumsum(column_name)` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(`Number of Programs`=1:8,\nFrequency = c(62,47,39,39,58,37,4,11),\nRelative.Frequency = c(62,47,39,39,58,37,4,11)/297) \n\nmutate(df,Cumulative.Relative.Frequency = cumsum(Relative.Frequency))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Number.of.Programs Frequency Relative.Frequency Cumulative.Relative.Frequency\n1                  1        62         0.20875421                     0.2087542\n2                  2        47         0.15824916                     0.3670034\n3                  3        39         0.13131313                     0.4983165\n4                  4        39         0.13131313                     0.6296296\n5                  5        58         0.19528620                     0.8249158\n6                  6        37         0.12457912                     0.9494949\n7                  7         4         0.01346801                     0.9629630\n8                  8        11         0.03703704                     1.0000000\n```\n\n\n:::\n:::\n\n### Graph\n\nThe plot of the cumulative distribution function is called an *ogive*. The option `stat_ecdf` provides us with a way to create ogives based on frequency values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df, aes(x=Number.of.Programs, y=Frequency)) +\n  stat_ecdf(geom=\"step\") + stat_ecdf(geom=\"point\",color=\"blue\",size=2) + # <1>\n  theme_bw() + \n  scale_x_continuous(breaks=c(1:8))+ # <2>\n  labs(x=\"Number of Programs\",y=\"F(x)\") \n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n1. \"step\" creates the lines in the ogive, while \"point\" shows the points corresponding to the values of F(x). The vertical lines are uninterpretable; they only provide a visual cue for the jump from one value to another.\n2. The `scale_x_continuous(breaks=c(1:8))` statement tells R to show all the points between 1 and 8 in the graph. This way, it is easier to interpret for other readers.\n\n\n:::\n\n## Example 2.1\n\nConsider the food assistance example. What is the probability of randomly selecting a family that have used less than three assistance programs.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf <- data.frame(`Number of Programs`=1:8,\nFrequency = c(62,47,39,39,58,37,4,11),\nRelative.Frequency = c(62,47,39,39,58,37,4,11)/297) \n\nmutate(df,Cumulative.Relative.Frequency = cumsum(Relative.Frequency))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Number.of.Programs Frequency Relative.Frequency Cumulative.Relative.Frequency\n1                  1        62         0.20875421                     0.2087542\n2                  2        47         0.15824916                     0.3670034\n3                  3        39         0.13131313                     0.4983165\n4                  4        39         0.13131313                     0.6296296\n5                  5        58         0.19528620                     0.8249158\n6                  6        37         0.12457912                     0.9494949\n7                  7         4         0.01346801                     0.9629630\n8                  8        11         0.03703704                     1.0000000\n```\n\n\n:::\n:::\n\n\n::: callout-tip\nLess than three assistance programs can also mean \"two or less programs\" or \"at most two programs\". Hence, we are interested in $F(2)$\n\n$F(2) = P(X \\leq 2) = P(X=1) + P(X=2)=$ 0.367\n:::\n\n## Exercise\n\nConsider the food assistance example.\n\n\n\n::: panel-tabset\n\n### Question\n\nWhat is the probability that a randomly selected family utilized at least 4 programs?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf <- data.frame(`Number of Programs`=1:8,\nFrequency = c(62,47,39,39,58,37,4,11),\nRelative.Frequency = c(62,47,39,39,58,37,4,11)/297) \n\nmutate(df,Cumulative.Relative.Frequency = cumsum(Relative.Frequency))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Number.of.Programs Frequency Relative.Frequency Cumulative.Relative.Frequency\n1                  1        62         0.20875421                     0.2087542\n2                  2        47         0.15824916                     0.3670034\n3                  3        39         0.13131313                     0.4983165\n4                  4        39         0.13131313                     0.6296296\n5                  5        58         0.19528620                     0.8249158\n6                  6        37         0.12457912                     0.9494949\n7                  7         4         0.01346801                     0.9629630\n8                  8        11         0.03703704                     1.0000000\n```\n\n\n:::\n:::\n\n\n\n### Answer\n\nThere are two ways to approach this question.\n\n::: callout-tip\nApproach 1: Sum the relative frequencies from $X=4$ to the maximum possible value ($X=8$).\n\n$p(4) + p(5) + p(6) + p(7) + p(8)$ = 0.1313+0.1953+0.1246+0.0135+0.0370 = 0.50167\n:::\n\n::: callout-tip\nApproach 2: Use the complement principle. To isolate the families who utilized at least 4 programs, we must remove those who utilized three programs or less. Then,\n\n$p(4) + p(5) + p(6) + p(7) + p(8)$ = 1-F(3) = 1-0.4983 = 0.5017\n:::\n\n:::\n\n\n## Expectation Values of Discrete Probability Distributions\n\nThe expected value of a random variable can be thought of as the average of a very large number of observations of the random variable. \n\n\n::: callout-tip\nThe expected value of $X$, denoted by $E(X)$, can be calculated by taking the sum of the products of each outcome \\{$x_1,x_2,...,$\\} and their corresponding probabilities \\{$p(x_1),p(x_2),...$\\}. In mathematical terms,\n\n$$\nE(X)= \\sum_i x_i p(x_i)\n$$\n:::\n\n::: callout-important\n$E(X)$ is also the **mean** of the discrete probability distribution, $\\mu$.\n:::\n\n## Example\n\nConsider a random variable $X$ that represents the outcome of a fair six-sided die. Prior to rolling the die, $X$ can take on any one of the six values: {1,2,3,4,5,6}. The probability distribution is shown below:\n\n\n::: panel-tabset\n\n### Probability Distribution\n\n| Outcome of $X$ | p(x) |\n|----------------|------|\n| 1              | 1/6  |\n| 2              | 1/6  |\n| 3              | 1/6  |\n| 4              | 1/6  |\n| 5              | 1/6  |\n| 6              | 1/6  |\n\n### Calculation\n\n$E(X)$ can be calculated using the formula as shown below:\n\n$$\nE(X) = 1(1/6)+2(1/6)+3(1/6)+4(1/6)+5(1/6)+6(1/6)\n$$\n\nThe result leads to $E(X)=$ 3.5 \n\n\n:::\n\n## Exercise\n\nConsider the food assistance example. What is the mean number of assistance programs used based on the PMF provided?\n\n::: panel-tabset\n\n### PMF\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  Number.of.Programs Frequency Relative.Frequency\n1                  1        62         0.20875421\n2                  2        47         0.15824916\n3                  3        39         0.13131313\n4                  4        39         0.13131313\n5                  5        58         0.19528620\n6                  6        37         0.12457912\n7                  7         4         0.01346801\n8                  8        11         0.03703704\n```\n\n\n:::\n:::\n\n\n### Answer\n\n$E(X) = \\sum_i x_i p(x_i)$ translates to: \n\nE(X) = 1(0.2087)+2(0.1582) + 3(0.1313) + 4(0.1313) + 5(0.1953) + 6(0.1246) + 7(0.0135) + 8(0.037).\n\nThis results to $E(X)=$ 3.559\n\n:::\n\n## Expectation Value of Functions\n\nExpectation values can also be calculated for functions. \n\n::: callout-tip\nThe expected value of a function $g(X)$, denoted by $E[g(X)]$, can be calculated by taking the sum of the products of each function evaluated at the outcome \\{$g(x_1),g(x_2),...,$\\} and their corresponding probabilities \\{$p(x_1),p(x_2),...$\\}. In mathematical terms,\n\n$$\nE[g(X)]= \\sum_i g(x_i) p(x_i)\n$$\n:::\n\n## Variance and Standard Deviation\n\nThe variance of the discrete probability distribution can be calculated using expectation values as well. The variance $\\sigma^2$ can be expressed as $E[(X-\\mu)^2]$. \n\n$$\n\\sigma^2 = E[(X-\\mu)^2] = \\sum_i (x_i-\\mu)^2 p(x_i)\n$$\n\nThe standard deviation can be calculated by taking taking the square root of the variance.\n\n$$\n\\sigma = \\sqrt{\\sigma^2}\n$$\n\n## Example\n\nWhat is the variance and standard deviation of the probability distribution for rolling a fair six-sided die?\n\n::: panel-tabset\n### Notes\n\nIn calculating the variance, it would be easier to use `R`. First, we would need to create a data frame for the probability distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(Outcome = 1:6,\n                 Prob = rep(1/6,6))\n\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Outcome      Prob\n1       1 0.1666667\n2       2 0.1666667\n3       3 0.1666667\n4       4 0.1666667\n5       5 0.1666667\n6       6 0.1666667\n```\n\n\n:::\n:::\n\n\n### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- sum(df$Outcome*df$Prob)\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.5\n```\n\n\n:::\n\n```{.r .cell-code}\nsigma_2 <- sum((df$Outcome-mu)^2*df$Prob)\nsigma_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.916667\n```\n\n\n:::\n:::\n\n\nThe variance is 2.9166667\n\n:::\n\n\n# Binomial Distribution\n\n## Bernoulli Trials\n\nA random process, also referred to as a trial, can result in only one of two mutually exclusive outcomes is called a **Bernoulli trial**.\n\n::: callout-note\nExamples of Bernoulli trials include dichotomous random variables such as mortality (dead/alive), coin tosses (heads/tails), and infections (infected/not infected).\n:::\n\n## Bernoulli Process\n\n::: callout-important\nA sequence of Bernoulli trials forms a *Bernoulli process*. Bernoulli processes have the following characteristics:\n\n- The experiment consists of exactly $n$ identical trials\n- Each trial can have only one of two outcomes (success/failure)\n- The probability of success, $p$ is constant for every trial. The probability of failure, $q$, is equal to $1-p$.\n- The trials are independent\n- The random variable of interest is the number of successes $X$ out of the $n$ trials.\n:::\n\n::: {.callout-note title=\"Example\"}\nImagine flipping a coin $n$ times. The random variable $X$ can be the number of heads observed after $n$ flips. \n\n- Success: Flipping a head, Failure: Flipping a tail\n- Probability of success, $p$ is 0.50 for a fair coin.\n:::\n\n## Binomial distribution\n\nThe random variable $X$ described in the Bernoulli process follows the Binomial distribution.\n\n::: {.callout-note title=\"Binomial Distribution\"}\nThe probability mass function (PMF) of a binomial random variable $X$ with parameters $n$ and $p$ is:\n\n$$\np(x) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\n$$\n:::\n\n::: callout-tip\nIf a random variable $X$ follows the binomial distribution with $n$ trials and success probability $p$, we can use the following notation: $X \\sim BIN(n,p)$\n:::\n\n::: callout-note\n$n!$ is the **factorial** of a non-negative integer $n$. The factorial is the product of all the positive integers less than or equal to n.\n\n$$\nn! = n(n-1)(n-2)...(2)(1)\n$$\n\nWith the special case $0!=1$\n:::\n\n## Dissecting the Binomial Distribution: Independent Events\n\nSuppose we toss a coin three times, each toss independent of each other. Assuming the probability of getting heads is $p$, the probability of flipping two heads out of three tosses can be calculated using the property of independent events.\n\n$$\nP(2 heads) = pp(1-p) = p^2 (1-p)^1 = p^2 (1-p)^{(3-2)}\n$$\n\n## Dissecting the Binomial Distribution: Counting Sample Points\n\nHowever, the probability measured using independent events does not account for the multiple configurations of flipping two heads out of three tosses.\n\n::: panel-tabset\n\n### Results\n\n::: callout-warning\nFlipping two heads out of three tosses could lead to the following results: HTH, HHT, THH. \n:::\n\n### Notes\n\nNote that the order in flipping the two heads does not matter; what matters is counting the number of ways that the two heads can occur after three tosses. These results are disjoint with the same probability $p^2(1-p)$. The probability that any of these results occur can be calculated using the addition rule:\n\n$$\nP(2heads) = P(HTH)+P(HHT) + P(THH)= 3p^2(1-p)\n$$\n:::\n\n\n## Combination\n\nHow do we account for the number of sample points when we consider 100 tosses instead of 3?\n\n::: {.callout-note title=\"Combination\"}\nThe unordered selection of $x$ successes out of $n$ trias is called a combination, also denoted by $C(n,x)$.\n\n$$\nC(n,x) = \\frac{n!}{x!(n-x)!}\n$$\n:::\n\n::: callout-tip\nUsing combinations for the three toss example, $n=3$, $x=2$.\n\n$$\nC(3,2) = \\frac{3!}{2!(3-2)!} = 3\n$$\n\n:::\n\n\n## Binomial Distribution in `R`\n\n`R` is handy for handling binomial random variables.\n\n::: panel-tabset\n\n### Combination\n\n`choose(n,x)` can calculate the number of combinations of $x$ successes in $n$ trials.\n\n### PMF\n\n`dbinom(x,n,p)` can calculate the PMF of the binomial distribution for an outcome of $x$ successes out of $n$ trials with success probability $p$.\n\n### CDF\n\n`pbinom(x,n,p)` can calculate the CDF of the binomial distribution for an outcome of $x$ successes out of $n$ trials with success probability $p$.\n\n### Random Binomial Numbers\n\n`rbinom(k,n,p)` generates `k` numbers for a binomial distribution with number of trials $n$ and success probability $p$. \n:::\n\n## Binomial Distribution: Visualization\n\n::: panel-tabset\n\n### BIN(10,0.5)\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n### BIN(10,0.7)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-12-1.pdf)\n:::\n:::\n\n\n### BIN(10,0.2)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-13-1.pdf)\n:::\n:::\n\n\n:::\n\n## Example\n\n\nSuppose we toss a fair coin ($p$=0.5) three times, each toss independent of each other.\n\n::: panel-tabset\n### Questions\n\n- What is the probability of flipping heads twice?\n\n  - Calculate manually using the equation for $p(x)$.\n  - Calculate using R.\n\n- What is the probability of flipping at most two heads? Calculate using `R`.\n- Simulate this binomial process 10 times using `rbinom()`\n\n### PMF (Math)\n\n$p(x) = C(3,2)(0.5)^2(1-0.5) = 3(0.5)^2(1-0.5)=$ 0.375\n\n### PMF (`R`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(2,3,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.375\n```\n\n\n:::\n:::\n\n\n### $P(X\\leq 2)$\n\n$P(X\\leq 2)$ corresponds to the value of the CDF at $x=2$ ($F(2)$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(2,3,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.875\n```\n\n\n:::\n\n```{.r .cell-code}\ndbinom(0,3,0.5) + dbinom(1,3,0.5) + dbinom(2,3,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.875\n```\n\n\n:::\n:::\n\nYou can also use the complementary principle. The complement of $X \\leq 2$ is $X=3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1- dbinom(3,3,0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.875\n```\n\n\n:::\n:::\n\n### Simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNsim<- 10 # Simulating 10 times\nset.seed(12) # <1>\nsims <- rbinom(Nsim,3,0.5)\nsims\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 2 3 1 1 0 1 2 0 0\n```\n\n\n:::\n:::\n\n1. Setting a seed enables us to draw the same numbers from the binomial distribution. \n\nWe can create a bar plot for the simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(simulations=sims)\nggplot(df, aes(x=simulations)) + \n  geom_histogram() + \n  theme_bw() + \n  labs(x=\"Number of Heads\", y=\"Count\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n:::\n\n## Exercise\n\nIn 2021, 8.9% of US adults were diagnosed with diabetes (Types I and II) [CDC](https://www.cdc.gov/diabetes/php/data-research/index.html).\n\n::: panel-tabset\n\n### Questions\n\nWhat is the probability that out of a sample of 250 adults in 2021,\n\n- Exactly 45 have diabetes\n- Between 30 and 60, inclusive, have diabetes\n- Simulate the sampling process 100 times and create a histogram of the count of participants diagnosed with diabetes out of 250 adults.\n\n### Answer\n\n- Exactly 45 have diabetes\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(45,250,0.089)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.638007e-06\n```\n\n\n:::\n:::\n\n\n\n- Between 30 and 60, inclusive, have diabetes: $30 \\leq x \\leq 60$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(60,250,0.089) - pbinom(29,250,0.089)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05825713\n```\n\n\n:::\n:::\n\n\n### Simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNsim<- 100 # Simulating 10 times\nset.seed(12) # <1>\nsims <- rbinom(Nsim,250,0.089)\ndf <- data.frame(simulations=sims)\nggplot(df, aes(x=simulations)) + \n  geom_histogram(binwidth=1) + \n  theme_bw() + \n  labs(x=\"Number of Diagnosed Diabetics\", y=\"Count\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n:::\n\n\n## Expected Value and Variance\n\n::: {.callout-note title=\"Expected Value\"}\nThe expected value/mean of the binomial distribution with $n$ trials and success probability $p$ is $E(X) = np$.\n:::\n\n::: {.callout-note title=\"Variance\"}\nThe variance of the binomial distribution with $n$ trials and success probability $p$ is $V(X) = np(1-p)$.\n:::\n\n## Example\n\nIn the Philippines, the estimated percentage of individuals who received booster shots for COVID-19 is 19.7%. \n\n::: panel-tabset\n\n### Question\n\nFor a sample of 2,500 Philippine residents, what is the expected value of Philippine residents who received a booster? What is the variance of the associated binomial distribution?\n\n### Answer\n\nExpected Value: $E(X) = 2500(0.197)=$ 492.5\n\nVariance: $V(X) = 2500(0.197)(1-0.197)=$ 395.4775\n\n:::\n\n## Exercise\n\nIn 2021, 8.9% of US adults were diagnosed with diabetes (Types I and II) [CDC](https://www.cdc.gov/diabetes/php/data-research/index.html).\n\n::: panel-tabset\n\n### Questions\n\nFor a sample of 250 adults in 2021, what is the expected number of diagnosed diabetics? What is the variance of the distribution?\n\n### Answers\n\nExpected Value: $E(X) = 250(0.089)=$ 22.25\n\nVariance: $V(X) = 250(0.089)(1-0.089)=$ 20.26975\n:::\n\n\n## Poisson Distribution\n\nThe Poisson distribution is often used to model the count of events occurring in an interval of time or space. These events are assumed to have a very low probability of occurrence in a small interval.\n\n::: callout-note\nExamples include number of cars stopped at an intersection for an hour, radioactive particles that decay in a given period of time, and number of hospitalizations in a day.\n:::\n\n## Poisson Process\n\nThe Poisson distribution results from a set of assumptions about an underlying process called the **Poisson process**.\n\n::: callout-important\nThe Poisson process has the following characteristics\n\n- The occurrence of an event in an interval of space or time has no effect on the probability of a second occurrence of the event in the same or other interval.\n- An infinite number of occurrences of the event must be possible in the interval\n- The probability of a single occurrence of the event in a given interval is proportional to the length of the interval.\n- In any infinitesimally small portion of the interval, the probability of more than one occurrence of the event is negligible.\n:::\n\n\n## Poisson Distribution\n\nThe PMF for the Poisson distribution is defined by the parameter $\\lambda$.\n\n$$\np(x) =\\frac{\\lambda^x e^{-\\lambda}}{x!}; x = 0,1,2,...\n$$\n\n::: callout-note\n$e$ is Euler's number, $\\lambda$ is the Poisson rate parameter describing the average number of events per unit interval.$\\lambda$ must be positive.\n\nThe notation for a Poisson distribution with a Poisson rate parameter $\\lambda$ is $X \\sim POIS(\\lambda)$.\n:::\n\n::: callout-important\nThe Poisson distribution can approximate a binomial distribution with large $n$ and small $p$.\n:::\n\n## Poisson Distribution: `R`\n\n`R` is also capable of calculating the PMF, CDF, and simulating data for a Poisson process.\n\n::: panel-tabset\n### Exponential\n\nThe `exp(lambda)` can calculate the $e^{-\\lambda}$ term in the Poisson PMF. \n\n### PMF\n\n`dpois(x,lambda)` calculates the value of the PMF for $X=x$, i.e. $P(X=x) = p(x)$.\n\n### CDF\n\n`ppois(x,lambda)` calculates the value of the CDF for $X=x$, i.e. $P(X\\leq x) = F(x)$.\n\n### Random \n\n`rpois(k,lambda)` draws `k` values from a random variable $X \\sim POIS(\\lambda)$.\n:::\n\n## Poisson Distribution: Visualization\n\n\n::: panel-tabset\n\n### POIS(1)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-22-1.pdf)\n:::\n:::\n\n\n### POIS(2.5)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-23-1.pdf)\n:::\n:::\n\n\n### POIS(5)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-24-1.pdf)\n:::\n:::\n\n:::\n\n## Example\n\nOn a weekday, the expected number of people in the waiting room of a small clinic is 14. \n\n::: panel-tabset\n\n### Questions\n\n- What is the probability that there will be exactly 20 people in the waiting room on a weekday?\n- What is the probability that there will be at most 10 people in the waiting room on a weekday?\n- Simulate 150 weekdays using `rpois()`. Create a histogram for the number of people in the waiting room from the 150 simulations.\n\n### Answers\n\n- Exactly 20 people\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndpois(20,14)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02859653\n```\n\n\n:::\n:::\n\n\n- At most 10 people\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppois(10,14)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1756812\n```\n\n\n:::\n:::\n\n### Simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNsim<- 150 # Simulating 10 times\nset.seed(12345) # <1>\nsims <- rpois(Nsim,14)\ndf <- data.frame(simulations=sims)\nggplot(df, aes(x=simulations)) + \n  geom_histogram(binwidth=1) + \n  theme_bw() + \n  labs(x=\"Number of People in the Waiting Room\", y=\"Count\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n## Exercise\n\nThe hourly number of pedestrians waiting by a bus stop is estimated to be 7.\n\n::: panel-tabset \n### Questions\n- What is the probability of observing exactly 7 pedestrians at the bus stop?\n- what is the probability of observing at least 10 pedestrians at the bus stop?\n- Draw 1000 values from the associated Poisson distribution using `rpois`. Create a histogram for the randomly drawn values.\n\n### Answers\n\n- Exactly 7 pedestrians\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndpois(7,7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1490028\n```\n\n\n:::\n:::\n\n\n- At least 10 pedestrians: Complement of at most 9 pedestrians! Hence, the probability is equal to $1-P(X \\leq 9) = 1-F(9)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1-ppois(9,7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1695041\n```\n\n\n:::\n:::\n\n\n### Simulations/Random Draws\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNsim<- 1000 # Simulating 10 times\nset.seed(12) # <1>\nsims <- rpois(Nsim,7)\ndf <- data.frame(simulations=sims)\nggplot(df, aes(x=simulations)) + \n  geom_histogram(binwidth=1) + \n  theme_bw() + \n  labs(x=\"Number of Pedestrians\", y=\"Count\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n## Expected Value and Variance\n\nThe expected value and variance of the Poisson distribution are equal and is given by the rate parameter $\\lambda$, i.e. $E(X) = \\lambda, V(X) = \\lambda$.\n\n\n::: callout-important\nIf we are interested in the expected value for $t$ units of time, then the expected value is given by $E(X) = \\lambda t$.\n:::\n\n::: callout-important\nThe equality between expected value and variance can be restrictive because data variability cannot be controlled such that this equality can hold.\n:::\n\n## Example\n\nThe hourly number of pedestrians waiting by a bus stop is estimated to be 7. \n\n::: panel-tabset\n\n### Questions\n\n- What is the expected value of the number of pedestrians waiting by the bus stop after one hour?\n- What is the variance of the associated Poisson distribution?\n- What is the expected value of the number of pedestrians waiting by the bus stop after 30 minutes?\n\n### Answers\n\n- After one hour, $E(X) = \\lambda t = (7)(1)=$ 7\n- Variance: $V(X) = \\lambda =$ 7\n- After 30 minutes, $E(X) = \\lambda t = (7)(0.5)=$ 3.5\n:::\n\n## Other Discrete Distributions\n\n::: {.callout-note title=\"Hypergeometric Distribution\"}\nConsider a population size $N$ and a sample size $n$. Each element in the population can be categorized into a \"success\" or failure. If there are $r$ successes in the population, the probability that $x$ successes will be drawn in a sample of size $n$ is given by the **hypergeometric distribution**.\n\nThis distribution is used in exact tests such as the Fisher's Exact Test.\n:::\n\n::: {.callout-note title=\"Negative Binomial Distribution\"}\nThe negative binomial distribution is used to model counts for overdispersed data and other biological processes. Overdispersion occurs when the data set exhibits more variability than what a statistical model expects. Recall that the Poisson distribution is restricted by the property that the mean is equal to the variance. \n:::\n\n# Probability Distributions of Continuous Probability Distributions\n\n## Continuous Random Variables\n\n::: {.callout-note title=\"Continuous Random Variable\"}\nA random variable is continuous if it can only take on values in one or more intervals of the real line.\n:::\n\n\n::: callout-tip\nTime, distance, weight, and volume are examples of continuous variables.\n:::\n\n## Continuous Probability Distributions\n\n::: callout-note\nRecall that probabilities from discrete random variables are found by summing PMF values.\n\n:::\n\nFor continuous random variables, summing becomes integration.\n\n::: callout-tip\nImagine creating a histogram with an infinite amount of narrow bins. Adding these values eventually lead to a sum of the areas under the curve.\n:::\n\n## Probability Density Functions\n\nThe continuous probability distribution can be expressed as a **probability density function (PDF)**, denoted by $f_X(x)$ or $f(x)$. \n\n::: callout-warning\nThere are important differences between probability density functions and probability mass functions (discrete). For example, for continuous variables, \n\n$P(X=x) = 0 \\neq f(x)$.\n\n:::\n\n\n## Cumulative Distribution Functions\n\nRecall that the cumulative distribution function (CDF) is given by $F_X(x) = P(X \\leq x)$. The CDF is mathematically defined as:\n\n$$\nF_X(x) = \\int_{-\\infty}^x f(x)dx\n$$\n\n::: callout-tip\n\nSince $P(X=x) = 0$, it also follows that\n\n$$\nP(a \\leq x \\leq b) = P(a < x \\leq b) = P(a \\leq x < b) =  P(a < x < b)\n$$\n\nand that\n\n$$\nF_X(x) = P(X \\leq x) = P(X<x)\n$$\n:::\n\n\n\n## Probabilities for Continuous Random Variables\n\nInstead of $P(X=x)$, we are more interested in the probability that a continuous random variable assumes a value between $a$ and $b$, denoted by $P(a < X < b)$, which can be calculated as:\n\n$$\nP(a < X < b) = \\int_a^b f(x)dx = F(b)-F(a)\n$$\n\n::: callout-note\nProbability density functions have the following properties\n\n- $f(x) \\geq 0$ for $-\\infty < x < \\infty$.\n- $\\int_{-\\infty}^{\\infty} f(x)dx = 1$ or $F(\\infty)=1$.\n:::\n\n## Uniform Distribution: PDF\n\n\nThe uniform distribution has a constant PDF such that\n\n\n$$\nf(x) = \\begin{cases} \n      \\frac{1}{\\theta_2-\\theta_1},~ \\theta_1 \\leq x \\leq \\theta_2; \\theta_2>\\theta_1 \\\\\n      0, ~ otherwise\n   \\end{cases}\n$$\n\nA random variable $X$ that has a uniform distribution on the interval $[\\theta_1,\\theta_2]$ is denoted as $X\\sim UNIF(\\theta_1,\\theta_2)$.\n\n::: callout-note\nA uniform distribution is defined by two parameters: $\\theta_1$ and $\\theta_2$. A standard uniform distribution is a special case such that $\\theta_1$=0, $\\theta_2$=1.\n:::\n\n## Uniform Distribution: Visualization\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-31-1.pdf)\n:::\n:::\n\n\n\n## Uniform Distribution: Probabilities\n\n::: callout-important\nLucky for you, there's no expectations of any calculus in class.\n:::\n\nThe probability $P(a<X<b)$ if $X\\sim UNIF(\\theta_1,\\theta_2)$  can be calculated using the following equation:\n\n$$\nP(a < X < b) = \\frac{b-a}{\\theta_2-\\theta_1}\n$$\n\nwhere $a,b$ are assumed to be between $\\theta_1 < a \\leq b < \\theta_2$.\n\n::: callout-important\nThis result can also be demonstrated using geometry!\n:::\n\n## Uniform Distribution: Probabilities in `R`\n\nIn `R`, `punif(x,min,max)` can calculate the probability $P(X \\leq x) = P(\\theta_1<X < x)$. Hence, if we want to calculate $P(a < X < b)$ when $X\\sim UNIF(\\theta_1,\\theta_2)$,\n\n```{.r}\nP(a < X < b) = punif(b,theta_1,theta_2)-punif(a,theta_1,theta_2)\n```\n\n::: callout-important\nYou can also draw random numbers from a uniform distribution $UNIF(\\theta_1,\\theta_2)$ using `runif(#sims,theta_1,theta_2)`\n:::\n\n## Example\n\nSuppose $X \\sim UNIF(0,5)$. What is the probability that an outcome $x$ is between 0.25 and 4?\n\n::: panel-tabset\n\n### Math\na = 0.25, b=4, $\\theta_1 = 0, \\theta_2=5$.\n\n$$\nP(a < X < b) = \\frac{b-a}{\\theta_2-\\theta_1} = \\frac{4-0.25}{5-0}\n$$\n\nHence, P(a < X < b) = 0.75\n\n### `R`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunif(4,0,5) - punif(0.25,0,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.75\n```\n\n\n:::\n:::\n\n:::\n\n## Exercise\n\nSuppose $X \\sim UNIF(0,100)$. \n\n::: panel-tabset\n\n### Question \n\nWhat is the probability that an outcome $x$ is between 55 and 85?\n\n### Answer\n\n$$\n\\frac{85-55}{100-0}=0.30\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunif(85,0,100) - punif(55,0,100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3\n```\n\n\n:::\n:::\n\n\n:::\n\n## Uniform Distribution: Expected Value and Variance\n\nFor the uniform distribution, the mean is given by $E(X) = \\frac{\\theta_1+\\theta_2}{2}$. The variance is given by $E(X) = \\frac{(\\theta_2+\\theta_1)^2}{12}$.\n\n## Gaussian Distribution\n\nThe Gaussian distribution, also known as the normal distribution, is an important distribution in statistics.\n\n::: callout-note\nMany natural physical and human-related phenomena closely follow a normal distribution.\n:::\n\n## Gaussian Distribution: PDF\n\nThe probability density function of the Gaussian distribution can be expressed as:\n\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}; -\\infty < x < \\infty\n$$\n\n::: callout-note\nA uniform distribution is defined by two parameters: the mean $\\mu$ and $\\sigma^2$. A random variable $X$ that has a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ is denoted as $X\\sim N(\\mu,\\sigma^2)$. \n:::\n\n::: callout-important\nA standard normal distribution is a special case such that $\\mu$=0, $\\sigma^2$=1. Variables that follow a standard normal distribution are often denoted as $Z$.    \n:::\n\n## Gaussian Distribution: Characteristics\n\nThe Gaussian distribution has the following characteristics:\n\n- The distribution is symmetric about the mean $\\mu$.\n- The mean, median, and mode are all equal.\n\n\n\n## Gaussian Distribution: Visualization\n\n::: panel-tabset\n\n### Same Means, Changing Variance\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-34-1.pdf)\n:::\n:::\n\n\n### Same Variance, Changing Means\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-35-1.pdf)\n:::\n:::\n\n\n\n:::\n\n## Gaussian Distribution: `R`\n\n::: callout-important\nCalculating probabilities mathematically would be impossible without calculus. However, `R` makes life easy for us.\n:::\n\n::: panel-tabset\n\n### PDF \n\nYou can calculate the PDF value for a normal distribution, f(x), using `dnorm(x,mean,sd)`. Note that the function uses standard deviation, not variance.\n\n### CDF \n\nYou can calculate the CDF value for a normal distribution, F(x), using `pnorm(x,mean,sd)`. Note that the function uses standard deviation, not variance.\n\n### Simulating/Generating Random Variables\n\nYou can simulate/generate random Gaussian variables using `rnorm(#sims,mean,sd)`. Note that the function uses standard deviation, not variance.\n\n### CDF to X/Z\n\nIf we know the value of $F(x)$, we can determine what $x$ is using `qnorm(F(x),mean, sd)`. If the mean and standard deviation/variance is unknown, we can get the corresponding value of the standard normal distribution $Z$ by specifying `mean=0` and `sd=1`.\n:::\n\n## Example\n\nFor a random variable $Z~N(0,1)$ following the standard normal distribution\n\n::: panel-tabset\n### Question\n\nCalculate the following probabilities using `R`\n\n- $P(Z \\leq 1)$\n- $P(Z \\geq 2.5)$\n- $P(-2 < Z < 2)$\n\n\n### Answer\n\n- $P(Z \\leq 1) = F(1)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(1,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8413447\n```\n\n\n:::\n:::\n\n\n- $P(Z\\geq 2.5) = 1- P(Z<2.5) = 1-F(2.5)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1-pnorm(2.5,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006209665\n```\n\n\n:::\n\n```{.r .cell-code}\npnorm(2.5,0,1,lower.tail=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006209665\n```\n\n\n:::\n:::\n\n\n- $P(-2 < Z < 2) = F(2)-F(-2)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(2,0,1) - pnorm(-2,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9544997\n```\n\n\n:::\n:::\n\n\n:::\n\n## Exercise\n\nConsider $X \\sim N(3,9)$.\n\n::: panel-tabset\n### Question\n\n- $P(X \\leq 6)$\n- $P(X \\geq 3)$\n- $P(-3 < X < 9)$\n\n### Answer\n\nNote that $\\sigma^2=9$, so sd=3.\n\n- $P(X \\leq 6)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(6,3,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8413447\n```\n\n\n:::\n:::\n\n\n- $P(X \\geq 10.5)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1- pnorm(10.5,3,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006209665\n```\n\n\n:::\n\n```{.r .cell-code}\npnorm(10.5,3,3,lower.tail=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.006209665\n```\n\n\n:::\n:::\n\n\n- $P(-3 < X < 9)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(9,3,3) - pnorm(-3,3,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9544997\n```\n\n\n:::\n:::\n\n\n::: callout-important\nNote that the answers are the same!\n:::\n\n:::\n\n## Other Continuous Distributions\n\n::: {.callout-note title=\"Weibull Distribution\"}\nThe Weibull distribution is used to model random variables that are constrained to be positive, such as lifetimes, impurities, etc. This is common for epidemic modeling and survival analysis.\n:::\n\n::: {.callout-note title=\"Statistical tests\"}\nContinuous distributions that you will learn about in future chapters are the $t$ distribution, $F$ distribution, and the $\\chi^2$ (chi-squared) distribution.\n:::\n\n# Application of the Gaussian/Normal Distribution\n\n## Standardization\n\nA common transformation employed with Gaussian distributed variables is standardization. Standardization involves rescaling the variable based on the mean, $\\mu$, and variance $\\sigma^2$, of the distribution. An observation $X$ can be standardized into a standardized variable $Z$ using the following equation:\n\n$$\nZ = \\frac{X-\\mu}{\\sigma}\n$$\n\n::: callout-important\nIf $X \\sim(\\mu,\\sigma^2)$, then $Z\\sim N(0,1)$. The CDF of the normal distribution is referred to by $\\Phi(z) = P(Z \\leq z)$.\n:::\n\n\n## Standard Normal Distribution: Implications\n\n::: callout-note\nThe random variable $Z$ calculates how many standard deviations away the observation is from the mean. The transformation also implies that\n\n$$\nP(a < X < b) = P \\left(\\frac{a-\\mu}{\\sigma} < Z < \\frac{b-\\mu}{\\sigma}\\right)\n$$\n\nand\n\n$$\nF(x) = \\Phi \\left(z=\\frac{x-\\mu}{\\sigma}\\right)\n$$\n:::\n\n\n## Example\n\nThe results of a biostatistics aptitude test was found to be normally distributed with a mean score of 78.5 and variance of\n120. \n\n- What is the probability that somebody scores above 92 points?\n- Generate 1000 draws from the corresponding normal distribution using `rnorm()` and create a histogram from the generated values.\n\n::: panel-tabset\n\n### No standardization\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(92,78.5,sqrt(120),lower.tail=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1089044\n```\n\n\n:::\n:::\n\n\n### Standardization\n\n$Z = \\frac{X-\\mu}{\\sigma}$ = 1.2323758\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm((92-78.5)/sqrt(120),0,1,lower.tail=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1089044\n```\n\n\n:::\n:::\n\n\n### Simulation/Generation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\ndf <- data.frame(x=rnorm(1000,mean=78.5,sd=sqrt(120)))\n\nggplot(df,aes(x=x)) + \n  geom_histogram(binwidth=1) +\n  theme_bw() + \n  labs(x=\"Generated Scores\",\n       y=\"Count\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-pdf/unnamed-chunk-44-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n## Exercise\n\nThe waiting time for physician appointments in a hospital was normally distributed with mean equal to 23.5 minutes and standard deviation of 6.7 minutes. What is the probability that a patient will wait less than five minutes for an appointment?\n\n::: panel-tabset\n\n### Question\n\n- Use standardization to calculate the standardized variable $z$ such that $P(X<5) = P(Z < z)$.\n\n- Calculate the probability using the standardization method.\n\n### Answer\n\n$z = \\frac{5-23.5}{6.7}$ = -2.761194\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm((5-23.5)/6.7,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002879523\n```\n\n\n:::\n:::\n\n\n:::\n\n## Implications\n\nFor every random variable $X \\sim N(\\mu,\\sigma^2)$, the probability of an outcome to be within one standard deviation of the mean can be written as:\n\n$$\nP((\\mu-\\sigma) < X < (\\mu+\\sigma)) = P(-1 < Z <1)\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(1,0,1) - pnorm(-1,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6826895\n```\n\n\n:::\n:::\n\n\n::: callout-note\nWe expect 68.3% of the measurements of the random variable $X$ is within one standard deviation of the mean ($\\pm 1\\sigma$)\n:::\n\n## Implications\n\nSimilarly, we can calculate the probability within $\\pm 2\\sigma$ and $\\pm 3\\sigma$. Around 95.4% of the outcomes are $\\pm 2\\sigma$, while 99.7% of the outcomes are $\\pm 3\\sigma$. This is why it is sometimes assumed that normally distributed data should be found within three standard deviations from the mean.\n\n::: panel-tabset\n\n### $\\pm 2 \\sigma$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(2,0,1) - pnorm(-2,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9544997\n```\n\n\n:::\n:::\n\n\n### $\\pm 3 \\sigma$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(3,0,1) - pnorm(-3,0,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9973002\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Calculating $z$ from $\\Phi(z)$\n\nThe function `qnorm(prob)` provides the value for $z$ such that the CDF $\\Phi(z) = P(Z \\leq z) = prob$.\n\n## Example\n\nFind $z$ such that $\\Phi(z) = 0.93$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.93)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.475791\n```\n\n\n:::\n:::\n\n\n## Example 2\n\nConsider a random distribution with mean 5 and standard deviation 2.7. What is the 90th percentile of the distribution?\n\n- The 90th percentile, $p_{90}$, can be expressed as $P(X \\leq p_{90}) = 0.90$. $p_{90}$ can be calculated using `qnorm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.90,mean=5,sd=2.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.460189\n```\n\n\n:::\n:::\n\n\n\n## Exercise\n\nSuppose the total cholesterol values for a certain population are approximately normally distributed with a mean of 205.7 mg/100ml with a standard deviation of 23.5 mg/100ml. \n\n::: panel-tabset\n### Questions\n\n- What is the probability that an individual picked at random will have\n\n  - Above 200 mg/100ml?\n  - Between 150 and 200 mg/100ml?\n\n- What is the cholesterol level such that only 5% of the population have cholesterol levels above this amount?\n \n\n### Answer\n\n- Above 200 mg/100ml\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(200,mean=205.7,sd=23.5,lower.tail=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5958242\n```\n\n\n:::\n:::\n\n\n- Between 150 and 200 mg/100ml\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(200,mean=205.7,sd=23.5) - pnorm(150,mean=205.7,sd=23.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3952868\n```\n\n\n:::\n:::\n\n\n- Only 5% of the population have cholesterol levels above this amount: $p_{95}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.95,mean=205.7,sd=23.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 244.3541\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n",
    "supporting": [
      "lecture4_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}